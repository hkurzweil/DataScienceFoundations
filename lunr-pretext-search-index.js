var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "sec-what-is-data-science",
  "level": "1",
  "url": "sec-what-is-data-science.html",
  "type": "Section",
  "number": "",
  "title": "What is Data Science?",
  "body": " What is Data Science?   Defining Data Science  Data science is an interdisciplinary field that combines domain knowledge, programming skills, and statistical techniques to extract meaningful insights from data. At its core, data science is about using data to answer questions, solve problems, and make decisions.   The Data Science Venn Diagram   A Venn diagram showing data science as the intersection of domain expertise, mathematics\/statistics, and computer science.    Software Engineering: A Practitioner's Approach    The field of data science has evolved from statistics, computer science, and specific domain applications. While statisticians have been analyzing data for centuries, modern data science incorporates newer tools and techniques to handle larger, more complex datasets and to gain deeper insights into the data to solve real world problems.   Data Science Concepts   Which of the following best describes data science?      An interdisciplinary field that combines domain knowledge, programming skills, and statistical techniques to extract insights from data    Correct! Data science combines multiple disciplines to analyze and interpret data.      A branch of computer science focused exclusively on machine learning algorithms    While machine learning is part of data science, data science is broader and includes other disciplines.      A synonym for statistics that uses modern technology    Statistics is a component of data science, but data science also incorporates programming, domain expertise, and other skills.      The process of creating visual representations of large datasets    Data visualization is an important part of data science, but data science encompasses much more.       Exploring Data Science History: Snow's Cholera Map   Long before the term \"data science\" existed, people were using data to solve important problems. One of the most famous historical examples is Dr. John Snow's investigation of a cholera outbreak in London in 1854.  During a severe cholera outbreak in the Soho district of London, Dr. Snow collected data on the locations of cholera deaths and plotted them on a map. By carefully mapping each case, he noticed a pattern: the deaths clustered around a specific water pump on Broad Street. This spatial analysis led him to hypothesize that contaminated water, not \"bad air\" (the prevailing theory at the time), was responsible for spreading the disease.   John Snow's Original Cholera Map (1854)   John Snow's map showing cholera cases clustered around the Broad Street pump in London.    National Geographic    Snow's methods included:    Data collection: Recording the locations of cholera deaths    Spatial analysis: Plotting cases on a map to identify patterns    Hypothesis testing: Testing his theory by investigating the water source    Intervention: Recommending the removal of the pump handle, which helped end the outbreak    This early example of data-driven decision making saved lives and helped establish the field of epidemiology.     How is Snow's approach similar to modern data science methods? Identify at least two similarities.      What limitations did Snow face in his data collection and analysis? How might these have affected his conclusions?      If Snow had access to modern data science tools (like CODAP, GIS systems, or statistical software), how might his analysis have been enhanced or expanded?       Why Data Literacy Matters  Data literacy—the ability to read, work with, analyze, and communicate with data—has become an essential skill in the 21st century. Whether you're making personal decisions, participating in civic discussions, or pursuing a career, understanding data helps you:    Make informed decisions based on evidence rather than intuition alone    Critically evaluate claims that others make using data    Communicate your own findings effectively    Identify misleading presentations of data    Understand complex systems and patterns in the world      Consider a news article that claims \"Violent crime has surged by 30% this year.\" A data-literate person would ask:    Compared to what baseline? (Last year? Five years ago? The lowest point ever recorded?)    What specific crimes are included in \"violent crime\"?    What geographic area does this apply to?    Has the way crimes are reported or recorded changed?    Is a percentage the most appropriate measure, or would raw numbers provide important context?       Data Literacy in the News   In this activity, we'll practice data literacy by examining news articles.     Go to Google and navigate to the News tab. Type in Data about and enter in a topic you're interested in. Choose an article that looks interesting to you.      Identify at least three questions you would ask to better understand the data behind the claim.      If possible, find the original data source and determine if the article's interpretation seems accurate.       Data Literacy Skills   Match each data literacy skill with its appropriate example.     Critical evaluation of claims  Questioning a news headline about crime statistics by examining the source data    Making informed decisions  Choosing between job offers by comparing salary and benefits data    Identifying misleading presentations  Noticing that a graph's y-axis doesn't start at zero, exaggerating differences    Effective communication of findings  Creating a clear visualization that shows the key trends in your research    Understanding complex patterns  Recognizing seasonal variations in multiple years of sales data      Applications of Data Science  Data science methods are transforming virtually every field:    Healthcare   Predicting disease outbreaks  Personalizing treatment plans  Improving diagnostic accuracy  Optimizing hospital operations     Business   Customer segmentation  Demand forecasting  Process optimization  Fraud detection       Environmental Science   Climate modeling  Ecosystem monitoring  Pollution tracking  Resource management     Social Sciences   Analyzing social networks  Studying behavioral patterns  Evaluating program effectiveness  Understanding demographic trends      Data Science in Your Field of Interest   In this activity, you'll explore how data science is used in a field that interests you.     Select a field or industry that interests you.      Research and identify at least three specific ways data science is being applied in this field.      For one application, describe the data that might be collected, how it might be analyzed, and what insights it provides.      "
},
{
  "id": "fig-data-science-venn",
  "level": "2",
  "url": "sec-what-is-data-science.html#fig-data-science-venn",
  "type": "Figure",
  "number": "1",
  "title": "",
  "body": " The Data Science Venn Diagram   A Venn diagram showing data science as the intersection of domain expertise, mathematics\/statistics, and computer science.    Software Engineering: A Practitioner's Approach   "
},
{
  "id": "mc-data-science-concepts",
  "level": "2",
  "url": "sec-what-is-data-science.html#mc-data-science-concepts",
  "type": "Checkpoint",
  "number": "2",
  "title": "Data Science Concepts.",
  "body": " Data Science Concepts   Which of the following best describes data science?      An interdisciplinary field that combines domain knowledge, programming skills, and statistical techniques to extract insights from data    Correct! Data science combines multiple disciplines to analyze and interpret data.      A branch of computer science focused exclusively on machine learning algorithms    While machine learning is part of data science, data science is broader and includes other disciplines.      A synonym for statistics that uses modern technology    Statistics is a component of data science, but data science also incorporates programming, domain expertise, and other skills.      The process of creating visual representations of large datasets    Data visualization is an important part of data science, but data science encompasses much more.     "
},
{
  "id": "activity-data-science-history",
  "level": "2",
  "url": "sec-what-is-data-science.html#activity-data-science-history",
  "type": "Activity",
  "number": "1",
  "title": "Exploring Data Science History: Snow’s Cholera Map.",
  "body": " Exploring Data Science History: Snow's Cholera Map   Long before the term \"data science\" existed, people were using data to solve important problems. One of the most famous historical examples is Dr. John Snow's investigation of a cholera outbreak in London in 1854.  During a severe cholera outbreak in the Soho district of London, Dr. Snow collected data on the locations of cholera deaths and plotted them on a map. By carefully mapping each case, he noticed a pattern: the deaths clustered around a specific water pump on Broad Street. This spatial analysis led him to hypothesize that contaminated water, not \"bad air\" (the prevailing theory at the time), was responsible for spreading the disease.   John Snow's Original Cholera Map (1854)   John Snow's map showing cholera cases clustered around the Broad Street pump in London.    National Geographic    Snow's methods included:    Data collection: Recording the locations of cholera deaths    Spatial analysis: Plotting cases on a map to identify patterns    Hypothesis testing: Testing his theory by investigating the water source    Intervention: Recommending the removal of the pump handle, which helped end the outbreak    This early example of data-driven decision making saved lives and helped establish the field of epidemiology.     How is Snow's approach similar to modern data science methods? Identify at least two similarities.      What limitations did Snow face in his data collection and analysis? How might these have affected his conclusions?      If Snow had access to modern data science tools (like CODAP, GIS systems, or statistical software), how might his analysis have been enhanced or expanded?    "
},
{
  "id": "example-data-literacy-everyday",
  "level": "2",
  "url": "sec-what-is-data-science.html#example-data-literacy-everyday",
  "type": "Example",
  "number": "4",
  "title": "",
  "body": "  Consider a news article that claims \"Violent crime has surged by 30% this year.\" A data-literate person would ask:    Compared to what baseline? (Last year? Five years ago? The lowest point ever recorded?)    What specific crimes are included in \"violent crime\"?    What geographic area does this apply to?    Has the way crimes are reported or recorded changed?    Is a percentage the most appropriate measure, or would raw numbers provide important context?     "
},
{
  "id": "activity-data-literacy-news",
  "level": "2",
  "url": "sec-what-is-data-science.html#activity-data-literacy-news",
  "type": "Activity",
  "number": "2",
  "title": "Data Literacy in the News.",
  "body": " Data Literacy in the News   In this activity, we'll practice data literacy by examining news articles.     Go to Google and navigate to the News tab. Type in Data about and enter in a topic you're interested in. Choose an article that looks interesting to you.      Identify at least three questions you would ask to better understand the data behind the claim.      If possible, find the original data source and determine if the article's interpretation seems accurate.    "
},
{
  "id": "matching-data-literacy",
  "level": "2",
  "url": "sec-what-is-data-science.html#matching-data-literacy",
  "type": "Checkpoint",
  "number": "5",
  "title": "Data Literacy Skills.",
  "body": " Data Literacy Skills   Match each data literacy skill with its appropriate example.     Critical evaluation of claims  Questioning a news headline about crime statistics by examining the source data    Making informed decisions  Choosing between job offers by comparing salary and benefits data    Identifying misleading presentations  Noticing that a graph's y-axis doesn't start at zero, exaggerating differences    Effective communication of findings  Creating a clear visualization that shows the key trends in your research    Understanding complex patterns  Recognizing seasonal variations in multiple years of sales data    "
},
{
  "id": "subsec-applications-3-1-1",
  "level": "2",
  "url": "sec-what-is-data-science.html#subsec-applications-3-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Healthcare "
},
{
  "id": "subsec-applications-3-2-1",
  "level": "2",
  "url": "sec-what-is-data-science.html#subsec-applications-3-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Business "
},
{
  "id": "subsec-applications-4-1-1",
  "level": "2",
  "url": "sec-what-is-data-science.html#subsec-applications-4-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Environmental Science "
},
{
  "id": "subsec-applications-4-2-1",
  "level": "2",
  "url": "sec-what-is-data-science.html#subsec-applications-4-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Social Sciences "
},
{
  "id": "activity-field-exploration",
  "level": "2",
  "url": "sec-what-is-data-science.html#activity-field-exploration",
  "type": "Activity",
  "number": "3",
  "title": "Data Science in Your Field of Interest.",
  "body": " Data Science in Your Field of Interest   In this activity, you'll explore how data science is used in a field that interests you.     Select a field or industry that interests you.      Research and identify at least three specific ways data science is being applied in this field.      For one application, describe the data that might be collected, how it might be analyzed, and what insights it provides.    "
},
{
  "id": "sec-intro-to-codap",
  "level": "1",
  "url": "sec-intro-to-codap.html",
  "type": "Section",
  "number": "",
  "title": "Introduction to CODAP",
  "body": " Introduction to CODAP  CODAP (Common Online Data Analysis Platform) is a free, web-based data analysis tool designed specifically for educational purposes. It provides an intuitive interface for exploring data without requiring programming knowledge.   The CODAP Interface  Let's explore the basic components of the CODAP interface:   The CODAP Interface   Screenshot of the CODAP interface showing the built-in Mammal example.    Key components include:    Tables : Display data in rows and columns    Graphs : Create visual representations of data    Calculator : Perform calculations and create new attributes    Map : Plot geographic data    Text : Add explanatory notes to your document    You can access CODAP at codap.concord.org . No login is required to get started, though you can save files to your local computer or link it to your Google drive, if you have one.    Your First CODAP Exploration  Let's dive into CODAP with a simple dataset.   CODAP Basics   In this activity, we'll explore a simple dataset in CODAP to get familiar with the interface.     Open CODAP in your web browser by going to codap.concord.org .      Click on \"Examples\" in the main menu and select \"Mammals\" from the list.      Explore the table by:    Scrolling through the data    Sorting columns by clicking on column headers    Resizing columns by dragging the column borders        Create a graph by:    Clicking on \"Graph\" in the main toolbar    Dragging \"Body Weight\" from the table to the horizontal axis    Dragging \"Brain Weight\" from the table to the vertical axis        Explore the relationship between body weight and brain weight by examining the graph. What patterns do you notice?     This basic exploration gives you a sense of how CODAP makes it easy to view and visualize data. Throughout this course, we'll build on these skills to perform more sophisticated analyses.   "
},
{
  "id": "fig-codap-interface",
  "level": "2",
  "url": "sec-intro-to-codap.html#fig-codap-interface",
  "type": "Figure",
  "number": "6",
  "title": "",
  "body": " The CODAP Interface   Screenshot of the CODAP interface showing the built-in Mammal example.   "
},
{
  "id": "subsec-codap-interface-5-1-1",
  "level": "2",
  "url": "sec-intro-to-codap.html#subsec-codap-interface-5-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Tables "
},
{
  "id": "subsec-codap-interface-5-2-1",
  "level": "2",
  "url": "sec-intro-to-codap.html#subsec-codap-interface-5-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Graphs "
},
{
  "id": "subsec-codap-interface-5-3-1",
  "level": "2",
  "url": "sec-intro-to-codap.html#subsec-codap-interface-5-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Calculator "
},
{
  "id": "subsec-codap-interface-5-4-1",
  "level": "2",
  "url": "sec-intro-to-codap.html#subsec-codap-interface-5-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Map "
},
{
  "id": "subsec-codap-interface-5-5-1",
  "level": "2",
  "url": "sec-intro-to-codap.html#subsec-codap-interface-5-5-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Text "
},
{
  "id": "activity-codap-basics",
  "level": "2",
  "url": "sec-intro-to-codap.html#activity-codap-basics",
  "type": "Activity",
  "number": "4",
  "title": "CODAP Basics.",
  "body": " CODAP Basics   In this activity, we'll explore a simple dataset in CODAP to get familiar with the interface.     Open CODAP in your web browser by going to codap.concord.org .      Click on \"Examples\" in the main menu and select \"Mammals\" from the list.      Explore the table by:    Scrolling through the data    Sorting columns by clicking on column headers    Resizing columns by dragging the column borders        Create a graph by:    Clicking on \"Graph\" in the main toolbar    Dragging \"Body Weight\" from the table to the horizontal axis    Dragging \"Brain Weight\" from the table to the vertical axis        Explore the relationship between body weight and brain weight by examining the graph. What patterns do you notice?    "
},
{
  "id": "sec-project-launch",
  "level": "1",
  "url": "sec-project-launch.html",
  "type": "Section",
  "number": "",
  "title": "Project Launch: Community Health and Environment",
  "body": " Project Launch: Community Health and Environment   The Sample Project  Throughout this course, we'll work with a Community Health and Environment dataset to demonstrate data science concepts. Versions can be found on the CDC's website where you'll navigate to the most recent file, click Export in the top right-hand corner, and download it as a .csv file. This dataset contains information about:    Health indicators (asthma rates, heart disease prevalence)    Environmental factors (air quality, green space access)    Demographic information (income, education, location)    Using this dataset, we'll explore questions such as:    How do environmental factors relate to health outcomes?    Do these relationships vary across different demographic groups?    What interventions might improve community health based on our findings?     Sample Community Health Dataset   Screenshot showing a preview of the community health and environment dataset with columns for neighborhood, health indicators, and environmental factors.      Selecting Your Own Dataset  While I'll demonstrate concepts using the Community Health dataset, you'll select and work with your own dataset throughout this course. Your chosen dataset should:    Contain at least 100 records (rows) of data    Include at least 8 variables (columns)    Include a mix of categorical and numerical data    Be complex enough to support interesting questions    Be available for download so you can work with the file    Here are some potential topics for your project:    Sports   Team or player statistics  Game outcomes  Performance metrics     Entertainment   Music streaming trends  Movie or TV ratings  Social media metrics     Local Issues   Traffic patterns  Business performance  Educational outcomes       Health & Wellness   Fitness tracking data  Nutrition databases  Public health statistics  Sleep pattern analysis     Environment & Climate   Weather patterns  Air\/water quality  Energy consumption  Biodiversity metrics     Economics & Finance   Consumer spending patterns  Housing market trends  Stock market data  Small business statistics       Technology & Innovation   Mobile app usage statistics  Technology adoption rates  Social media metrics  Internet accessibility worldwide     Transportation   Public transit ridership data  Vehicle safety statistics  Bike sharing program usage  Commuting patterns     Education & Learning   Student performance metrics  Higher education statistics  Educational technology usage  Learning outcomes by teaching method      Dataset Exploration   In this activity, you'll explore potential datasets for your project.     Visit at least two of these data repositories:    Kaggle Datasets    Data.gov    Gapminder    Data.world        Identify three potential datasets that match our criteria and interest you.      For each dataset, record in a working file for this project:    The source and a brief description    The number of records and variables    At least two questions you might investigate with this dataset       To stay on time for this project, you should have selected your dataset and begun exploring it in CODAP by the end of this week.   "
},
{
  "id": "fig-sample-dataset",
  "level": "2",
  "url": "sec-project-launch.html#fig-sample-dataset",
  "type": "Figure",
  "number": "8",
  "title": "",
  "body": " Sample Community Health Dataset   Screenshot showing a preview of the community health and environment dataset with columns for neighborhood, health indicators, and environmental factors.   "
},
{
  "id": "subsec-student-projects-5-1-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-5-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Sports "
},
{
  "id": "subsec-student-projects-5-2-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-5-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Entertainment "
},
{
  "id": "subsec-student-projects-5-3-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-5-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Local Issues "
},
{
  "id": "subsec-student-projects-6-1-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-6-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Health & Wellness "
},
{
  "id": "subsec-student-projects-6-2-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-6-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Environment & Climate "
},
{
  "id": "subsec-student-projects-6-3-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-6-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Economics & Finance "
},
{
  "id": "subsec-student-projects-7-1-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-7-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Technology & Innovation "
},
{
  "id": "subsec-student-projects-7-2-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-7-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Transportation "
},
{
  "id": "subsec-student-projects-7-3-1",
  "level": "2",
  "url": "sec-project-launch.html#subsec-student-projects-7-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Education & Learning "
},
{
  "id": "activity-dataset-exploration",
  "level": "2",
  "url": "sec-project-launch.html#activity-dataset-exploration",
  "type": "Activity",
  "number": "5",
  "title": "Dataset Exploration.",
  "body": " Dataset Exploration   In this activity, you'll explore potential datasets for your project.     Visit at least two of these data repositories:    Kaggle Datasets    Data.gov    Gapminder    Data.world        Identify three potential datasets that match our criteria and interest you.      For each dataset, record in a working file for this project:    The source and a brief description    The number of records and variables    At least two questions you might investigate with this dataset      "
},
{
  "id": "sec-data-types",
  "level": "1",
  "url": "sec-data-types.html",
  "type": "Section",
  "number": "",
  "title": "Understanding Data Types",
  "body": " Understanding Data Types   Qualitative vs. Quantitative Data  One of the most fundamental distinctions in data is between qualitative and quantitative data:     Qualitative data (also called categorical data) represents characteristics that can be observed but not measured numerically. Qualitative data can be classified into categories.       Quantitative data represents information that can be measured numerically and can be used in calculations.      Consider the following variables from our Community Health dataset:    Qualitative : Neighborhood name, zip code, predominant housing type    Quantitative : Asthma rate (%), air quality index, median household income ($), number of parks       Identifying Data Types   In this activity, you'll practice identifying qualitative and quantitative data.     For the sample Community Health dataset, identify whether each variable is qualitative or quantitative:   Neighborhood  Population density  Predominant land use  Percent green space  Average temperature (°F)  Healthcare access rating       Now examine your chosen dataset and classify each variable as qualitative or quantitative.       Identifying Data Types   Which of the following is an example of quantitative data?      Hair color    Hair color is qualitative (categorical) data as it represents a characteristic that can be categorized but not measured numerically.      Zip code    Although zip codes contain numbers, they are actually qualitative data because they represent categories (geographic regions) rather than measurements.      Height in centimeters    Correct! Height in centimeters is quantitative data because it represents a numerical measurement that can be used in calculations.      Blood type    Blood type (A, B, AB, O) is qualitative data as it represents categories rather than numerical measurements.       Measurement Scales  Data can be further classified by measurement scale, which affects what operations and analyses make sense for that data.     Nominal data represents categories with no inherent order. The only valid operation is determining equality (same or different).       Ordinal data represents categories with a meaningful order or ranking, but the differences between values may not be consistent or meaningful.       Interval data has consistent differences between values, but lacks a meaningful zero point.       Ratio data has consistent differences between values and a meaningful zero point (zero represents the absence of the quantity).      Examples from our Community Health dataset:    Nominal : Neighborhood name, predominant land use (residential, commercial, industrial, mixed)    Ordinal : Healthcare access rating (poor, fair, good, excellent), air quality category (unhealthy, moderate, good)    Interval : Temperature (°F or °C) - the difference between 70°F and 80°F is the same as between 80°F and 90°F, but 0°F doesn't represent the absence of temperature    Ratio : Asthma rate (%), income ($), number of parks, population - zero means none, and the difference between values is consistent       Understanding measurement scales helps you determine appropriate:   Summary statistics (mean, median, mode)  Visualization methods (bar charts, histograms, scatter plots)  Analysis techniques (correlation, regression, categorical tests)     Classifying by Measurement Scale   In this activity, you'll practice identifying measurement scales.     For each variable below, identify its measurement scale (nominal, ordinal, interval, or ratio):   ZIP code  Education level (no high school, high school, bachelor's, graduate)  Annual rainfall (inches)  Birth month  Satisfaction rating (1-5 scale)  Distance from city center (miles)       Now examine your chosen dataset and classify each variable by measurement scale.       Measurement Scales   Match each variable with its appropriate measurement scale.     Temperature in degrees Fahrenheit  Interval    Brand preference (Favorite soda brand)  Nominal    Customer satisfaction rating (1-5 stars)  Ordinal    Height in centimeters  Ratio    Academic letter grades (A, B, C, D, F)  Ordinal    Weight in kilograms  Ratio      Other Data Classifications  Beyond the qualitative\/quantitative and measurement scale distinctions, there are other useful ways to classify data:     Discrete data can only take specific values (usually whole numbers), while continuous data can take any value within a range.      From our Community Health dataset:    Discrete : Number of parks, number of healthcare facilities, population count    Continuous : Air quality index, median income, percent green space, asthma rate         Structured data is organized in a predefined format (like a spreadsheet or database), while unstructured data doesn't conform to a predefined data model (text, images, audio).    In this course, we'll primarily work with structured data, but it's important to know that unstructured data constitutes the majority of data generated today.   Data Type Review   Which of the following statements about data types and classifications is correct?      All numerical data is quantitative and all text data is qualitative.    Not all numerical data is quantitative. For example, zip codes are numerical but are considered qualitative (nominal) data.      Nominal and ordinal scales are types of quantitative data.    Nominal and ordinal scales are actually types of qualitative (categorical) data.      Interval data has a true zero point, while ratio data does not.    This is reversed. Ratio data has a true zero point (representing the absence of the quantity), while interval data does not.      Discrete data can only take specific values, while continuous data can take any value within a range.    Correct! Discrete data is limited to specific values (usually whole numbers), while continuous data can take any value within a range, including fractional values.         Tidy data is a specific way of organizing data where:   Each variable forms a column  Each observation forms a row  Each type of observational unit forms a table      Untidy Data   The untidy data format has values that are spread across multiple columns.     Tidy Data   The tidy data format has variables as columns and observations as rows.    Tidy data makes analysis and visualization more straightforward, as most analysis tools (including CODAP) are designed to work with data in this format.   Identifying Tidy Data   In this activity, you'll practice identifying tidy and untidy data.     Examine the following datasets in CODAP:    Open a new CODAP document    Click on \"Example Documents\" in the main menu    Open both \"Mammals\" and \"Speed Trap\"        For each dataset, determine if it follows the principles of tidy data. If not, explain what would need to change to make it tidy.      Examine your chosen project dataset. Is it in tidy format? If not, what would need to change?      "
},
{
  "id": "def-qualitative-data",
  "level": "2",
  "url": "sec-data-types.html#def-qualitative-data",
  "type": "Definition",
  "number": "9",
  "title": "",
  "body": "   Qualitative data (also called categorical data) represents characteristics that can be observed but not measured numerically. Qualitative data can be classified into categories.   "
},
{
  "id": "def-quantitative-data",
  "level": "2",
  "url": "sec-data-types.html#def-quantitative-data",
  "type": "Definition",
  "number": "10",
  "title": "",
  "body": "   Quantitative data represents information that can be measured numerically and can be used in calculations.   "
},
{
  "id": "example-qual-quant",
  "level": "2",
  "url": "sec-data-types.html#example-qual-quant",
  "type": "Example",
  "number": "11",
  "title": "",
  "body": "  Consider the following variables from our Community Health dataset:    Qualitative : Neighborhood name, zip code, predominant housing type    Quantitative : Asthma rate (%), air quality index, median household income ($), number of parks     "
},
{
  "id": "activity-identify-data-types",
  "level": "2",
  "url": "sec-data-types.html#activity-identify-data-types",
  "type": "Activity",
  "number": "6",
  "title": "Identifying Data Types.",
  "body": " Identifying Data Types   In this activity, you'll practice identifying qualitative and quantitative data.     For the sample Community Health dataset, identify whether each variable is qualitative or quantitative:   Neighborhood  Population density  Predominant land use  Percent green space  Average temperature (°F)  Healthcare access rating       Now examine your chosen dataset and classify each variable as qualitative or quantitative.    "
},
{
  "id": "mc-data-types",
  "level": "2",
  "url": "sec-data-types.html#mc-data-types",
  "type": "Checkpoint",
  "number": "12",
  "title": "Identifying Data Types.",
  "body": " Identifying Data Types   Which of the following is an example of quantitative data?      Hair color    Hair color is qualitative (categorical) data as it represents a characteristic that can be categorized but not measured numerically.      Zip code    Although zip codes contain numbers, they are actually qualitative data because they represent categories (geographic regions) rather than measurements.      Height in centimeters    Correct! Height in centimeters is quantitative data because it represents a numerical measurement that can be used in calculations.      Blood type    Blood type (A, B, AB, O) is qualitative data as it represents categories rather than numerical measurements.     "
},
{
  "id": "def-nominal",
  "level": "2",
  "url": "sec-data-types.html#def-nominal",
  "type": "Definition",
  "number": "13",
  "title": "",
  "body": "   Nominal data represents categories with no inherent order. The only valid operation is determining equality (same or different).   "
},
{
  "id": "def-ordinal",
  "level": "2",
  "url": "sec-data-types.html#def-ordinal",
  "type": "Definition",
  "number": "14",
  "title": "",
  "body": "   Ordinal data represents categories with a meaningful order or ranking, but the differences between values may not be consistent or meaningful.   "
},
{
  "id": "def-interval",
  "level": "2",
  "url": "sec-data-types.html#def-interval",
  "type": "Definition",
  "number": "15",
  "title": "",
  "body": "   Interval data has consistent differences between values, but lacks a meaningful zero point.   "
},
{
  "id": "def-ratio",
  "level": "2",
  "url": "sec-data-types.html#def-ratio",
  "type": "Definition",
  "number": "16",
  "title": "",
  "body": "   Ratio data has consistent differences between values and a meaningful zero point (zero represents the absence of the quantity).   "
},
{
  "id": "example-measurement-scales",
  "level": "2",
  "url": "sec-data-types.html#example-measurement-scales",
  "type": "Example",
  "number": "17",
  "title": "",
  "body": "  Examples from our Community Health dataset:    Nominal : Neighborhood name, predominant land use (residential, commercial, industrial, mixed)    Ordinal : Healthcare access rating (poor, fair, good, excellent), air quality category (unhealthy, moderate, good)    Interval : Temperature (°F or °C) - the difference between 70°F and 80°F is the same as between 80°F and 90°F, but 0°F doesn't represent the absence of temperature    Ratio : Asthma rate (%), income ($), number of parks, population - zero means none, and the difference between values is consistent     "
},
{
  "id": "subsec-measurement-scales-8",
  "level": "2",
  "url": "sec-data-types.html#subsec-measurement-scales-8",
  "type": "Insight",
  "number": "18",
  "title": "",
  "body": " Understanding measurement scales helps you determine appropriate:   Summary statistics (mean, median, mode)  Visualization methods (bar charts, histograms, scatter plots)  Analysis techniques (correlation, regression, categorical tests)   "
},
{
  "id": "activity-measurement-scales",
  "level": "2",
  "url": "sec-data-types.html#activity-measurement-scales",
  "type": "Activity",
  "number": "7",
  "title": "Classifying by Measurement Scale.",
  "body": " Classifying by Measurement Scale   In this activity, you'll practice identifying measurement scales.     For each variable below, identify its measurement scale (nominal, ordinal, interval, or ratio):   ZIP code  Education level (no high school, high school, bachelor's, graduate)  Annual rainfall (inches)  Birth month  Satisfaction rating (1-5 scale)  Distance from city center (miles)       Now examine your chosen dataset and classify each variable by measurement scale.    "
},
{
  "id": "matching-measurement-scales",
  "level": "2",
  "url": "sec-data-types.html#matching-measurement-scales",
  "type": "Checkpoint",
  "number": "19",
  "title": "Measurement Scales.",
  "body": " Measurement Scales   Match each variable with its appropriate measurement scale.     Temperature in degrees Fahrenheit  Interval    Brand preference (Favorite soda brand)  Nominal    Customer satisfaction rating (1-5 stars)  Ordinal    Height in centimeters  Ratio    Academic letter grades (A, B, C, D, F)  Ordinal    Weight in kilograms  Ratio    "
},
{
  "id": "def-discrete-continuous",
  "level": "2",
  "url": "sec-data-types.html#def-discrete-continuous",
  "type": "Definition",
  "number": "20",
  "title": "",
  "body": "   Discrete data can only take specific values (usually whole numbers), while continuous data can take any value within a range.   "
},
{
  "id": "example-discrete-continuous",
  "level": "2",
  "url": "sec-data-types.html#example-discrete-continuous",
  "type": "Example",
  "number": "21",
  "title": "",
  "body": "  From our Community Health dataset:    Discrete : Number of parks, number of healthcare facilities, population count    Continuous : Air quality index, median income, percent green space, asthma rate     "
},
{
  "id": "def-structured-unstructured",
  "level": "2",
  "url": "sec-data-types.html#def-structured-unstructured",
  "type": "Definition",
  "number": "22",
  "title": "",
  "body": "   Structured data is organized in a predefined format (like a spreadsheet or database), while unstructured data doesn't conform to a predefined data model (text, images, audio).   "
},
{
  "id": "mc-data-type-review",
  "level": "2",
  "url": "sec-data-types.html#mc-data-type-review",
  "type": "Checkpoint",
  "number": "23",
  "title": "Data Type Review.",
  "body": " Data Type Review   Which of the following statements about data types and classifications is correct?      All numerical data is quantitative and all text data is qualitative.    Not all numerical data is quantitative. For example, zip codes are numerical but are considered qualitative (nominal) data.      Nominal and ordinal scales are types of quantitative data.    Nominal and ordinal scales are actually types of qualitative (categorical) data.      Interval data has a true zero point, while ratio data does not.    This is reversed. Ratio data has a true zero point (representing the absence of the quantity), while interval data does not.      Discrete data can only take specific values, while continuous data can take any value within a range.    Correct! Discrete data is limited to specific values (usually whole numbers), while continuous data can take any value within a range, including fractional values.     "
},
{
  "id": "def-tidy-data",
  "level": "2",
  "url": "sec-data-types.html#def-tidy-data",
  "type": "Definition",
  "number": "24",
  "title": "",
  "body": "   Tidy data is a specific way of organizing data where:   Each variable forms a column  Each observation forms a row  Each type of observational unit forms a table    "
},
{
  "id": "fig-untidy-data",
  "level": "2",
  "url": "sec-data-types.html#fig-untidy-data",
  "type": "Figure",
  "number": "25",
  "title": "",
  "body": " Untidy Data   The untidy data format has values that are spread across multiple columns.   "
},
{
  "id": "fig-tidy-data",
  "level": "2",
  "url": "sec-data-types.html#fig-tidy-data",
  "type": "Figure",
  "number": "26",
  "title": "",
  "body": " Tidy Data   The tidy data format has variables as columns and observations as rows.   "
},
{
  "id": "activity-tidy-data",
  "level": "2",
  "url": "sec-data-types.html#activity-tidy-data",
  "type": "Activity",
  "number": "8",
  "title": "Identifying Tidy Data.",
  "body": " Identifying Tidy Data   In this activity, you'll practice identifying tidy and untidy data.     Examine the following datasets in CODAP:    Open a new CODAP document    Click on \"Example Documents\" in the main menu    Open both \"Mammals\" and \"Speed Trap\"        For each dataset, determine if it follows the principles of tidy data. If not, explain what would need to change to make it tidy.      Examine your chosen project dataset. Is it in tidy format? If not, what would need to change?    "
},
{
  "id": "sec-ethics-spotlight-data-collection",
  "level": "1",
  "url": "sec-ethics-spotlight-data-collection.html",
  "type": "Section",
  "number": "",
  "title": "Ethics Spotlight: Ethical Data Collection",
  "body": " Ethics Spotlight: Ethical Data Collection  Before we conclude Unit 1, it's important to consider the ethical dimensions of data science. The data we collect and how we collect it has real impacts on people's lives.  Key ethical considerations in data collection include:    Consent : Were people informed about how their data would be used?    Privacy : Is sensitive information protected?    Representativeness : Does the data collection process exclude certain groups?    Transparency : Is the collection process clear and documented?    Minimization : Is only necessary data collected?      In our Community Health dataset, consider these ethical questions:    Are the neighborhoods defined in ways that might reinforce historical segregation?    Does the dataset include communities that are often underrepresented in research?    Is the aggregation level appropriate to protect individual privacy while still being useful?    Were community members involved in deciding what data to collect about their neighborhoods?       Data Ethics Discussion   In this activity, we'll discuss ethical considerations for our datasets.     For the Community Health dataset, identify at least two potential ethical concerns and how you might address them.      For your chosen project dataset, consider:    What was the original purpose of this data collection?    Who collected it and how?    Who might be missing from or underrepresented in this dataset?    Are there privacy concerns with this data?       "
},
{
  "id": "sec-ethics-spotlight-data-collection-4-1-1",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#sec-ethics-spotlight-data-collection-4-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Consent "
},
{
  "id": "sec-ethics-spotlight-data-collection-4-2-1",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#sec-ethics-spotlight-data-collection-4-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Privacy "
},
{
  "id": "sec-ethics-spotlight-data-collection-4-3-1",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#sec-ethics-spotlight-data-collection-4-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Representativeness "
},
{
  "id": "sec-ethics-spotlight-data-collection-4-4-1",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#sec-ethics-spotlight-data-collection-4-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Transparency "
},
{
  "id": "sec-ethics-spotlight-data-collection-4-5-1",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#sec-ethics-spotlight-data-collection-4-5-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Minimization "
},
{
  "id": "example-ethics-health",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#example-ethics-health",
  "type": "Example",
  "number": "28",
  "title": "",
  "body": "  In our Community Health dataset, consider these ethical questions:    Are the neighborhoods defined in ways that might reinforce historical segregation?    Does the dataset include communities that are often underrepresented in research?    Is the aggregation level appropriate to protect individual privacy while still being useful?    Were community members involved in deciding what data to collect about their neighborhoods?     "
},
{
  "id": "activity-ethics-discussion",
  "level": "2",
  "url": "sec-ethics-spotlight-data-collection.html#activity-ethics-discussion",
  "type": "Activity",
  "number": "9",
  "title": "Data Ethics Discussion.",
  "body": " Data Ethics Discussion   In this activity, we'll discuss ethical considerations for our datasets.     For the Community Health dataset, identify at least two potential ethical concerns and how you might address them.      For your chosen project dataset, consider:    What was the original purpose of this data collection?    Who collected it and how?    Who might be missing from or underrepresented in this dataset?    Are there privacy concerns with this data?      "
},
{
  "id": "sec-unit1-summary",
  "level": "1",
  "url": "sec-unit1-summary.html",
  "type": "Section",
  "number": "",
  "title": "Unit 1 Summary",
  "body": " Unit 1 Summary  In this unit, we've explored:    What data science is and why data literacy matters    Applications of data science across different domains    The basics of CODAP, our data analysis platform    The Community Health sample project and guidelines for your own project    Different data types and classification systems    Ethical considerations in data collection    In Unit 2, we'll dive deeper into the data investigation process, learning how to formulate meaningful questions and plan our analysis.   Unit 1 Reflection   Take some time to reflect on what you've learned in this unit:    What aspect of data science interests you most and why?    What surprised you about the different types of data and their classifications?    How might data science skills be valuable in your current or future career?    What questions do you have about your project dataset so far?      "
},
{
  "id": "exercise-unit1-reflection",
  "level": "2",
  "url": "sec-unit1-summary.html#exercise-unit1-reflection",
  "type": "Checkpoint",
  "number": "30",
  "title": "Unit 1 Reflection.",
  "body": " Unit 1 Reflection   Take some time to reflect on what you've learned in this unit:    What aspect of data science interests you most and why?    What surprised you about the different types of data and their classifications?    How might data science skills be valuable in your current or future career?    What questions do you have about your project dataset so far?     "
},
{
  "id": "sec-investigation-framework",
  "level": "1",
  "url": "sec-investigation-framework.html",
  "type": "Section",
  "number": "",
  "title": "The Data Investigation Framework",
  "body": " The Data Investigation Framework   Overview of the Framework  Hollylynne Lee and her colleagues at North Carolina State University have developed a comprehensive framework for data investigations in their paper Investigating Data Like a Scientist: Key Practices and Processes that helps structure the process of working with data. This framework, seen below, provides a roadmap that can guide us through the complexity of real-world data analysis.   The Data Investigation Cycle (based on Lee et al.)   Team at NCSU's graphic for the data investigation process framework.    HIRISE Project, NC State University    The framework consists of four interconnected phases:    Ask Questions  Formulate statistical questions that can be answered with data. These questions should anticipate variability and focus on distributions rather than individual cases.    Consider Data  Evaluate available data sources, collection methods, and variables. Consider what additional data might be needed and assess the quality and appropriateness of the data for answering your questions.    Analyze Data  Apply appropriate techniques to organize, summarize, and visualize the data. This includes cleaning the data, creating meaningful representations, and identifying patterns.    Interpret Results  Make claims based on the evidence from your analysis, acknowledge limitations, and consider implications. This often leads to new questions, continuing the cycle.    While the framework is presented as a cycle, real investigations rarely follows that circle perfectly. You might need to revisit earlier phases as you gain insights or encounter challenges. Iteration is essential in data analysis.   Data Investigation Phases   During which phase of the data investigation framework would you typically clean the dataset by handling missing values?      Ask Questions    The Ask Questions phase focuses on formulating statistical questions, not on cleaning data.      Consider Data    While you might identify data quality issues during the Consider Data phase, actual cleaning typically happens during analysis.      Analyze Data    Correct! Data cleaning is part of the Analyze Data phase, where you prepare and transform the data for meaningful analysis.      Interpret Results    The Interpret Results phase focuses on drawing conclusions from the analysis, not on cleaning the data.        The Framework in Practice  Let's see how this framework might apply to our Community Health and Environment project:   Community Health Investigation    Ask Questions : How does air quality relate to asthma rates across different neighborhoods? Do these relationships differ based on income levels?   Consider Data : Our dataset includes air quality measurements, asthma prevalence rates, and median household income for various neighborhoods. We need to consider how air quality was measured, whether measurements were taken consistently, and if there are confounding variables we should account for.   Analyze Data : We might create scatterplots of air quality versus asthma rates, calculate correlation coefficients, and group neighborhoods by income levels to compare patterns. We'd also need to handle any missing values or outliers.   Interpret Results : Based on our analysis, we might find that neighborhoods with poorer air quality tend to have higher asthma rates, and this relationship could be stronger in lower-income areas. We'd need to acknowledge limitations (correlation doesn't imply causation) and consider implications for public health policy.    The framework helps ensure that we approach data investigations systematically, considering crucial aspects at each stage. It also highlights the iterative nature of data analysis—insights from later phases often prompt us to refine our questions or seek additional data.   Applying the Framework   In this activity, you'll apply the data investigation framework to your chosen project dataset.     For your chosen dataset, brainstorm at least three potential statistical questions you could investigate.      For one of your questions, outline what you would do in each phase of the data investigation framework.      Identify at least one challenge you might face in each phase and how you might address it.      Framework Activities   Match each data science activity with the most appropriate phase of the data investigation framework.     Creating a histogram of income distribution  Analyze Data    Determining whether survey respondents are representative of the population  Consider Data    Wondering if exercise habits differ by age group  Ask Questions    Concluding that the data doesn't support the original hypothesis  Interpret Results    Deciding to collect additional measurements  Consider Data    Calculating the correlation between temperature and energy use  Analyze Data      "
},
{
  "id": "fig-lee-framework",
  "level": "2",
  "url": "sec-investigation-framework.html#fig-lee-framework",
  "type": "Figure",
  "number": "31",
  "title": "",
  "body": " The Data Investigation Cycle (based on Lee et al.)   Team at NCSU's graphic for the data investigation process framework.    HIRISE Project, NC State University   "
},
{
  "id": "mc-framework-phases",
  "level": "2",
  "url": "sec-investigation-framework.html#mc-framework-phases",
  "type": "Checkpoint",
  "number": "32",
  "title": "Data Investigation Phases.",
  "body": " Data Investigation Phases   During which phase of the data investigation framework would you typically clean the dataset by handling missing values?      Ask Questions    The Ask Questions phase focuses on formulating statistical questions, not on cleaning data.      Consider Data    While you might identify data quality issues during the Consider Data phase, actual cleaning typically happens during analysis.      Analyze Data    Correct! Data cleaning is part of the Analyze Data phase, where you prepare and transform the data for meaningful analysis.      Interpret Results    The Interpret Results phase focuses on drawing conclusions from the analysis, not on cleaning the data.     "
},
{
  "id": "example-framework-health",
  "level": "2",
  "url": "sec-investigation-framework.html#example-framework-health",
  "type": "Example",
  "number": "33",
  "title": "Community Health Investigation.",
  "body": " Community Health Investigation    Ask Questions : How does air quality relate to asthma rates across different neighborhoods? Do these relationships differ based on income levels?   Consider Data : Our dataset includes air quality measurements, asthma prevalence rates, and median household income for various neighborhoods. We need to consider how air quality was measured, whether measurements were taken consistently, and if there are confounding variables we should account for.   Analyze Data : We might create scatterplots of air quality versus asthma rates, calculate correlation coefficients, and group neighborhoods by income levels to compare patterns. We'd also need to handle any missing values or outliers.   Interpret Results : Based on our analysis, we might find that neighborhoods with poorer air quality tend to have higher asthma rates, and this relationship could be stronger in lower-income areas. We'd need to acknowledge limitations (correlation doesn't imply causation) and consider implications for public health policy.   "
},
{
  "id": "activity-apply-framework",
  "level": "2",
  "url": "sec-investigation-framework.html#activity-apply-framework",
  "type": "Activity",
  "number": "10",
  "title": "Applying the Framework.",
  "body": " Applying the Framework   In this activity, you'll apply the data investigation framework to your chosen project dataset.     For your chosen dataset, brainstorm at least three potential statistical questions you could investigate.      For one of your questions, outline what you would do in each phase of the data investigation framework.      Identify at least one challenge you might face in each phase and how you might address it.    "
},
{
  "id": "matching-framework-activities",
  "level": "2",
  "url": "sec-investigation-framework.html#matching-framework-activities",
  "type": "Checkpoint",
  "number": "34",
  "title": "Framework Activities.",
  "body": " Framework Activities   Match each data science activity with the most appropriate phase of the data investigation framework.     Creating a histogram of income distribution  Analyze Data    Determining whether survey respondents are representative of the population  Consider Data    Wondering if exercise habits differ by age group  Ask Questions    Concluding that the data doesn't support the original hypothesis  Interpret Results    Deciding to collect additional measurements  Consider Data    Calculating the correlation between temperature and energy use  Analyze Data    "
},
{
  "id": "sec-statistical-questions",
  "level": "1",
  "url": "sec-statistical-questions.html",
  "type": "Section",
  "number": "",
  "title": "Formulating Statistical Questions",
  "body": " Formulating Statistical Questions   What Makes a Question Statistical?  Not all questions are statistical questions. A statistical question is one that can be answered by collecting data and where we expect variability in that data.    A statistical question is a question that anticipates variability in the data related to it and can be answered by analyzing data. It usually addresses patterns, trends, or relationships in a group or population rather than specific individuals.     Statistical vs. Non-Statistical Questions   Non-statistical questions typically have a single, deterministic answer:    How tall is Jamal? (Asks about a specific individual)    What is the capital of France? (Has a definitive answer)    Did it rain yesterday? (Yes\/no factual question)    Statistical questions anticipate variability and focus on distributions:    How tall are 7th-grade students at Lincoln Middle School? (Expects variation in heights)    What is the relationship between a country's GDP and its literacy rate? (Examines patterns across countries)    How does rainfall vary by month in Seattle? (Looks at distribution over time)       Identifying Statistical Questions   Which of the following is a statistical question?      What is the temperature right now?    This is not a statistical question because it asks for a single measurement at a specific point in time, not a distribution or pattern.      How many siblings does Maria have?    This is not a statistical question because it asks about a specific individual and has a single answer.      How does commute time vary among employees at a company?    Correct! This is a statistical question because it anticipates variability (different commute times) and requires analyzing data from a group.      Is 15 minutes a long time to wait for a bus?    This is not a statistical question because it asks for a subjective judgment rather than something that can be answered directly with data.      Good statistical questions are the foundation of effective data investigations. They guide the entire process, from data collection to analysis and interpretation.    Crafting Effective Statistical Questions  Effective statistical questions have several key characteristics:    Clear and specific : Precisely defines what you want to know and what population you're studying    Answerable with data : Can be investigated through data collection and analysis    Anticipates variability : Expects a distribution of values rather than a single answer    Meaningful : Addresses something worth investigating and has potential implications    Neutral : Doesn't presuppose a particular answer or bias the investigation     Refining Statistical Questions   Consider how we might refine these initial questions to make them more effective:    Initial : Is air pollution bad for health?  Refined : What is the relationship between average annual air quality index (AQI) and asthma hospitalization rates across neighborhoods in our city over the past five years?    Initial : Do parks make neighborhoods healthier?  Refined : How does the percentage of green space in a neighborhood correlate with residents' self-reported physical activity levels, controlling for median income?    Initial : Which neighborhood has the worst environmental health?  Refined : How do neighborhoods compare across multiple environmental health indicators (air quality, water quality, access to green space, and proximity to pollution sources), and what patterns emerge when considering demographic factors?      Notice how the refined questions are more specific about what is being measured, the population being studied, and the relationships being investigated. They also avoid loaded terms like \"bad\" or \"worst\" that might bias the investigation.   Refining Your Questions   In this activity, you'll work on developing effective statistical questions for your project.     Review the statistical questions you brainstormed in the previous activity. Select one that you think has the most potential.      Refine your selected question using the characteristics of effective statistical questions. Make it clearer, more specific, and more answerable with data.      Share your refined question with a classmate and provide feedback on each other's questions.      Improving Statistical Questions   For each initial question below, select the most improved version that follows the principles of effective statistical questions.     Question 1: Initial question: Are students getting enough sleep?  Select the most improved version of this question:    a . Should students get more sleep?    b . What is the distribution of nightly sleep duration among high school students, and how does it compare to recommended amounts for adolescents?    c . Why don't students get enough sleep?    d . Who sleeps the most in the sophomore class?    Question 2: Initial question: Does income affect health?  Select the most improved version of this question:    a . Is it fair that rich people are healthier?    b . What is John's income and health status?    c . How do rates of chronic diseases vary across different income brackets in the United States, and has this relationship changed over the past decade?    d . Why do poor people have worse health outcomes?     Think about what makes a good statistical question: it should be specific, measurable, and free from assumptions or value judgments.             Question 1: The best improved version is: What is the distribution of nightly sleep duration among high school students, and how does it compare to recommended amounts for adolescents?  This question is specific about what is being measured (sleep duration), the population (high school students), and includes a comparison to a standard.  Question 2: The best improved version is: How do rates of chronic diseases vary across different income brackets in the United States, and has this relationship changed over the past decade?  This question is specific about the variables (chronic disease rates, income brackets), the population (United States), and adds a time dimension for additional insight.         "
},
{
  "id": "def-statistical-question",
  "level": "2",
  "url": "sec-statistical-questions.html#def-statistical-question",
  "type": "Definition",
  "number": "35",
  "title": "",
  "body": "  A statistical question is a question that anticipates variability in the data related to it and can be answered by analyzing data. It usually addresses patterns, trends, or relationships in a group or population rather than specific individuals.   "
},
{
  "id": "example-stat-vs-nonstat",
  "level": "2",
  "url": "sec-statistical-questions.html#example-stat-vs-nonstat",
  "type": "Example",
  "number": "36",
  "title": "Statistical vs. Non-Statistical Questions.",
  "body": " Statistical vs. Non-Statistical Questions   Non-statistical questions typically have a single, deterministic answer:    How tall is Jamal? (Asks about a specific individual)    What is the capital of France? (Has a definitive answer)    Did it rain yesterday? (Yes\/no factual question)    Statistical questions anticipate variability and focus on distributions:    How tall are 7th-grade students at Lincoln Middle School? (Expects variation in heights)    What is the relationship between a country's GDP and its literacy rate? (Examines patterns across countries)    How does rainfall vary by month in Seattle? (Looks at distribution over time)     "
},
{
  "id": "mc-identify-stat-questions",
  "level": "2",
  "url": "sec-statistical-questions.html#mc-identify-stat-questions",
  "type": "Checkpoint",
  "number": "37",
  "title": "Identifying Statistical Questions.",
  "body": " Identifying Statistical Questions   Which of the following is a statistical question?      What is the temperature right now?    This is not a statistical question because it asks for a single measurement at a specific point in time, not a distribution or pattern.      How many siblings does Maria have?    This is not a statistical question because it asks about a specific individual and has a single answer.      How does commute time vary among employees at a company?    Correct! This is a statistical question because it anticipates variability (different commute times) and requires analyzing data from a group.      Is 15 minutes a long time to wait for a bus?    This is not a statistical question because it asks for a subjective judgment rather than something that can be answered directly with data.     "
},
{
  "id": "subsec-crafting-questions-3-1-1",
  "level": "2",
  "url": "sec-statistical-questions.html#subsec-crafting-questions-3-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Clear and specific "
},
{
  "id": "subsec-crafting-questions-3-2-1",
  "level": "2",
  "url": "sec-statistical-questions.html#subsec-crafting-questions-3-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Answerable with data "
},
{
  "id": "subsec-crafting-questions-3-3-1",
  "level": "2",
  "url": "sec-statistical-questions.html#subsec-crafting-questions-3-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Anticipates variability "
},
{
  "id": "subsec-crafting-questions-3-4-1",
  "level": "2",
  "url": "sec-statistical-questions.html#subsec-crafting-questions-3-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Meaningful "
},
{
  "id": "subsec-crafting-questions-3-5-1",
  "level": "2",
  "url": "sec-statistical-questions.html#subsec-crafting-questions-3-5-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Neutral "
},
{
  "id": "example-refining-questions",
  "level": "2",
  "url": "sec-statistical-questions.html#example-refining-questions",
  "type": "Example",
  "number": "38",
  "title": "Refining Statistical Questions.",
  "body": " Refining Statistical Questions   Consider how we might refine these initial questions to make them more effective:    Initial : Is air pollution bad for health?  Refined : What is the relationship between average annual air quality index (AQI) and asthma hospitalization rates across neighborhoods in our city over the past five years?    Initial : Do parks make neighborhoods healthier?  Refined : How does the percentage of green space in a neighborhood correlate with residents' self-reported physical activity levels, controlling for median income?    Initial : Which neighborhood has the worst environmental health?  Refined : How do neighborhoods compare across multiple environmental health indicators (air quality, water quality, access to green space, and proximity to pollution sources), and what patterns emerge when considering demographic factors?     "
},
{
  "id": "activity-refine-questions",
  "level": "2",
  "url": "sec-statistical-questions.html#activity-refine-questions",
  "type": "Activity",
  "number": "11",
  "title": "Refining Your Questions.",
  "body": " Refining Your Questions   In this activity, you'll work on developing effective statistical questions for your project.     Review the statistical questions you brainstormed in the previous activity. Select one that you think has the most potential.      Refine your selected question using the characteristics of effective statistical questions. Make it clearer, more specific, and more answerable with data.      Share your refined question with a classmate and provide feedback on each other's questions.    "
},
{
  "id": "exercise-improve-questions",
  "level": "2",
  "url": "sec-statistical-questions.html#exercise-improve-questions",
  "type": "Checkpoint",
  "number": "39",
  "title": "Improving Statistical Questions.",
  "body": " Improving Statistical Questions   For each initial question below, select the most improved version that follows the principles of effective statistical questions.     Question 1: Initial question: Are students getting enough sleep?  Select the most improved version of this question:    a . Should students get more sleep?    b . What is the distribution of nightly sleep duration among high school students, and how does it compare to recommended amounts for adolescents?    c . Why don't students get enough sleep?    d . Who sleeps the most in the sophomore class?    Question 2: Initial question: Does income affect health?  Select the most improved version of this question:    a . Is it fair that rich people are healthier?    b . What is John's income and health status?    c . How do rates of chronic diseases vary across different income brackets in the United States, and has this relationship changed over the past decade?    d . Why do poor people have worse health outcomes?     Think about what makes a good statistical question: it should be specific, measurable, and free from assumptions or value judgments.             Question 1: The best improved version is: What is the distribution of nightly sleep duration among high school students, and how does it compare to recommended amounts for adolescents?  This question is specific about what is being measured (sleep duration), the population (high school students), and includes a comparison to a standard.  Question 2: The best improved version is: How do rates of chronic diseases vary across different income brackets in the United States, and has this relationship changed over the past decade?  This question is specific about the variables (chronic disease rates, income brackets), the population (United States), and adds a time dimension for additional insight.       "
},
{
  "id": "sec-planning-investigation",
  "level": "1",
  "url": "sec-planning-investigation.html",
  "type": "Section",
  "number": "",
  "title": "Planning an Investigation",
  "body": " Planning an Investigation   Elements of an Investigation Plan  A well-structured data investigation plan helps guide your analysis and ensures you consider all important aspects before diving into the data. Key elements of an investigation plan include:    Research Questions  The statistical questions you aim to answer, clearly stated and refined.    Data Requirements  The specific variables, measurements, and data sources needed to answer your questions.    Data Assessment  Evaluation of the available data's quality, completeness, and appropriateness for your questions.    Analysis Approach  The methods and techniques you plan to use for data cleaning, exploration, visualization, and statistical analysis.    Potential Challenges  Anticipated difficulties and limitations, along with strategies to address them.    Expected Outcomes  What you hope to learn and how you might apply the findings.     Organizing an Investigation Plan   Arrange the following steps in a logical order for planning a data investigation.     Formulate clear statistical questions based on your area of interest.     Begin analyzing the data immediately to save time.    Identify what specific variables and data you need to answer your questions.    Create visualizations before examining the data quality.     Assess whether the available data is suitable and of sufficient quality.    Determine appropriate methods for cleaning, visualizing, and analyzing the data.    Identify potential limitations and challenges in your investigation.    Skip directly to writing conclusions without analyzing the data.     Think about what information you need to know before you can determine your analysis methods.    A thoughtful investigation plan serves as a roadmap for your analysis, though you should be prepared to adapt it as you learn more about the data and discover unexpected patterns or challenges.    Sample Investigation Plan  Let's look at a sample investigation plan for our Community Health and Environment project:   Community Health Investigation Plan    Research Questions :   What is the relationship between air quality index (AQI) and asthma rates across neighborhoods?    How does access to green space correlate with obesity rates, controlling for neighborhood income?    Are there clusters of neighborhoods with similar environmental health profiles, and how do these clusters relate to demographic factors?      Data Requirements :   Neighborhood-level data on air quality measurements (annual average AQI)    Asthma prevalence rates by neighborhood    Green space access metrics (percentage of area, proximity to parks)    Obesity rates by neighborhood    Median household income by neighborhood    Additional environmental health indicators (water quality, proximity to pollution sources)    Demographic information (age distribution, racial\/ethnic composition)      Data Assessment :   Our dataset includes most required variables but lacks detailed green space metrics    Some neighborhoods have missing data for certain health indicators    Air quality measurements were taken at different times of year across neighborhoods    Need to verify that health data is age-adjusted for fair comparison      Analysis Approach :   Data cleaning: Handle missing values, check for outliers, standardize variables    Exploratory analysis: Create scatter plots, histograms, and maps to visualize distributions and relationships    Calculate correlation coefficients between environmental factors and health outcomes    Perform regression analysis to examine relationships while controlling for income    Use cluster analysis to identify neighborhoods with similar environmental health profiles      Potential Challenges :   Missing data might bias results if not handled appropriately    Correlation doesn't imply causation; many confounding variables might exist    Neighborhood boundaries might not align perfectly with environmental exposure patterns    Limited sample size (number of neighborhoods) might affect statistical power      Expected Outcomes :   Identification of environmental factors most strongly associated with health outcomes    Understanding of how socioeconomic factors interact with environmental exposures    Visualization of environmental health disparities across the city    Insights that could inform targeted public health interventions       This plan provides a comprehensive framework for the investigation, identifying key questions, necessary data, analytical approaches, and potential limitations. It serves as a guide but remains flexible enough to adapt as the investigation proceeds.   Developing Your Investigation Plan   In this activity, you'll develop an investigation plan for your own dataset.     Using the refined statistical questions from the previous activity, outline a complete investigation plan following the structure of the sample plan.      Assess your dataset critically. Does it contain all the variables you need? Are there quality issues you need to address?      Specify at least three analysis techniques you plan to use and why they're appropriate for your research questions.      Identifying Data Requirements   To investigate the question \"How does public transportation usage vary with income level across neighborhoods?\", which of the following variables would be LEAST essential?      Average number of public transportation trips per resident by neighborhood    This is a key measure of public transportation usage, which is central to the research question.      Median household income by neighborhood    Income level is explicitly mentioned in the research question, making this variable essential.      Average home value by neighborhood    Correct! While home value might correlate with income, it's not directly mentioned in the research question and is less essential than variables that directly measure transportation usage and income.      Availability of public transportation (e.g., bus stops per square mile) by neighborhood    This is important to consider because availability can influence usage, making it a potential confounding variable that should be accounted for.       "
},
{
  "id": "parsons-investigation-plan",
  "level": "2",
  "url": "sec-planning-investigation.html#parsons-investigation-plan",
  "type": "Checkpoint",
  "number": "40",
  "title": "Organizing an Investigation Plan.",
  "body": " Organizing an Investigation Plan   Arrange the following steps in a logical order for planning a data investigation.     Formulate clear statistical questions based on your area of interest.     Begin analyzing the data immediately to save time.    Identify what specific variables and data you need to answer your questions.    Create visualizations before examining the data quality.     Assess whether the available data is suitable and of sufficient quality.    Determine appropriate methods for cleaning, visualizing, and analyzing the data.    Identify potential limitations and challenges in your investigation.    Skip directly to writing conclusions without analyzing the data.     Think about what information you need to know before you can determine your analysis methods.   "
},
{
  "id": "example-health-plan",
  "level": "2",
  "url": "sec-planning-investigation.html#example-health-plan",
  "type": "Example",
  "number": "41",
  "title": "Community Health Investigation Plan.",
  "body": " Community Health Investigation Plan    Research Questions :   What is the relationship between air quality index (AQI) and asthma rates across neighborhoods?    How does access to green space correlate with obesity rates, controlling for neighborhood income?    Are there clusters of neighborhoods with similar environmental health profiles, and how do these clusters relate to demographic factors?      Data Requirements :   Neighborhood-level data on air quality measurements (annual average AQI)    Asthma prevalence rates by neighborhood    Green space access metrics (percentage of area, proximity to parks)    Obesity rates by neighborhood    Median household income by neighborhood    Additional environmental health indicators (water quality, proximity to pollution sources)    Demographic information (age distribution, racial\/ethnic composition)      Data Assessment :   Our dataset includes most required variables but lacks detailed green space metrics    Some neighborhoods have missing data for certain health indicators    Air quality measurements were taken at different times of year across neighborhoods    Need to verify that health data is age-adjusted for fair comparison      Analysis Approach :   Data cleaning: Handle missing values, check for outliers, standardize variables    Exploratory analysis: Create scatter plots, histograms, and maps to visualize distributions and relationships    Calculate correlation coefficients between environmental factors and health outcomes    Perform regression analysis to examine relationships while controlling for income    Use cluster analysis to identify neighborhoods with similar environmental health profiles      Potential Challenges :   Missing data might bias results if not handled appropriately    Correlation doesn't imply causation; many confounding variables might exist    Neighborhood boundaries might not align perfectly with environmental exposure patterns    Limited sample size (number of neighborhoods) might affect statistical power      Expected Outcomes :   Identification of environmental factors most strongly associated with health outcomes    Understanding of how socioeconomic factors interact with environmental exposures    Visualization of environmental health disparities across the city    Insights that could inform targeted public health interventions      "
},
{
  "id": "activity-develop-plan",
  "level": "2",
  "url": "sec-planning-investigation.html#activity-develop-plan",
  "type": "Activity",
  "number": "12",
  "title": "Developing Your Investigation Plan.",
  "body": " Developing Your Investigation Plan   In this activity, you'll develop an investigation plan for your own dataset.     Using the refined statistical questions from the previous activity, outline a complete investigation plan following the structure of the sample plan.      Assess your dataset critically. Does it contain all the variables you need? Are there quality issues you need to address?      Specify at least three analysis techniques you plan to use and why they're appropriate for your research questions.    "
},
{
  "id": "mc-data-requirements",
  "level": "2",
  "url": "sec-planning-investigation.html#mc-data-requirements",
  "type": "Checkpoint",
  "number": "42",
  "title": "Identifying Data Requirements.",
  "body": " Identifying Data Requirements   To investigate the question \"How does public transportation usage vary with income level across neighborhoods?\", which of the following variables would be LEAST essential?      Average number of public transportation trips per resident by neighborhood    This is a key measure of public transportation usage, which is central to the research question.      Median household income by neighborhood    Income level is explicitly mentioned in the research question, making this variable essential.      Average home value by neighborhood    Correct! While home value might correlate with income, it's not directly mentioned in the research question and is less essential than variables that directly measure transportation usage and income.      Availability of public transportation (e.g., bus stops per square mile) by neighborhood    This is important to consider because availability can influence usage, making it a potential confounding variable that should be accounted for.     "
},
{
  "id": "sec-statistical-thinking",
  "level": "1",
  "url": "sec-statistical-thinking.html",
  "type": "Section",
  "number": "",
  "title": "Statistical Thinking: Understanding Variability",
  "body": " Statistical Thinking: Understanding Variability   Key Concepts in Variability  Statistical thinking is a way of approaching problems that acknowledges and accounts for variability. Understanding variability is essential for interpreting data and making informed decisions.     Variability refers to the extent to which data points in a set differ from each other and from measures of central tendency. It is an inherent characteristic of data that arises from multiple sources.    Several key concepts help us understand and quantify variability:    Measures of Center  Statistics like mean, median, and mode that describe typical values in a distribution.    Measures of Spread  Statistics like range, interquartile range, variance, and standard deviation that quantify the amount of variability.    Distribution Shape  The pattern of variation in a dataset, which might be symmetric, skewed, uniform, bimodal, or follow a specific distribution like normal.    Sources of Variability  Factors that contribute to differences in observations, including natural variation, measurement error, and sampling methods.     Visualizing Variability in Distributions   Three histograms showing distributions with different variability: low, medium, and high. All have the same mean but different spread.     Understanding Variability   Which of the following statements about variability is TRUE?      High variability in a dataset always indicates a problem with data collection.    This is not true. High variability might be a natural characteristic of the phenomenon being studied, not necessarily an indication of problems with data collection.      The mean is always the best measure of central tendency regardless of variability.    This is not true. When data is highly skewed or contains outliers, the median may be a more appropriate measure of central tendency.      Two datasets with the same mean must have the same standard deviation.    This is not true. Datasets with the same mean can have vastly different spread or variability, resulting in different standard deviations.      Understanding variability is essential for making appropriate inferences from data.    Correct! Acknowledging and accounting for variability is a fundamental aspect of statistical thinking and is crucial for proper data interpretation.        Accounting for Variability in Data Analysis  When conducting data investigations, we need to consider variability at every stage:    When asking questions : Formulate questions that acknowledge and explore variability    When considering data : Evaluate how sampling methods might affect variability    When analyzing data : Use appropriate visualizations and statistics to represent variability    When interpreting results : Consider how variability affects the strength and reliability of conclusions     Variability in Community Health Data   In our Community Health dataset, we might observe that:    Asthma rates vary considerably across neighborhoods (spatial variability)    Air quality measurements fluctuate seasonally (temporal variability)    The relationship between green space and obesity is stronger in some demographic groups than others (variability in relationships)    Some health metrics have greater measurement uncertainty than others (variability due to measurement)    Acknowledging these sources of variability helps us avoid oversimplified conclusions and recognize the complexity of environmental health relationships.     Exploring Variability in Your Dataset   In this activity, you'll explore sources of variability in your chosen dataset.     Identify at least three variables in your dataset and use CODAP to create visualizations showing their distributions.      For each variable, describe the pattern of variability you observe. Is the distribution symmetric, skewed, uniform, bimodal, or something else?      Consider what might cause the variability you're seeing. Is it natural variation in the phenomenon, differences between groups, measurement issues, or something else?      Measures of Variability   Match each statistical measure with the aspect of variability it best represents.     Standard deviation  Average distance of values from the mean    Interquartile range (IQR)  Spread of the middle 50% of the data    Range  Difference between the maximum and minimum values    Variance  Average of squared deviations from the mean    Coefficient of variation  Standard deviation relative to the mean      "
},
{
  "id": "def-variability",
  "level": "2",
  "url": "sec-statistical-thinking.html#def-variability",
  "type": "Definition",
  "number": "43",
  "title": "",
  "body": "   Variability refers to the extent to which data points in a set differ from each other and from measures of central tendency. It is an inherent characteristic of data that arises from multiple sources.   "
},
{
  "id": "fig-variability-visualized",
  "level": "2",
  "url": "sec-statistical-thinking.html#fig-variability-visualized",
  "type": "Figure",
  "number": "44",
  "title": "",
  "body": " Visualizing Variability in Distributions   Three histograms showing distributions with different variability: low, medium, and high. All have the same mean but different spread.   "
},
{
  "id": "mc-variability-concepts",
  "level": "2",
  "url": "sec-statistical-thinking.html#mc-variability-concepts",
  "type": "Checkpoint",
  "number": "45",
  "title": "Understanding Variability.",
  "body": " Understanding Variability   Which of the following statements about variability is TRUE?      High variability in a dataset always indicates a problem with data collection.    This is not true. High variability might be a natural characteristic of the phenomenon being studied, not necessarily an indication of problems with data collection.      The mean is always the best measure of central tendency regardless of variability.    This is not true. When data is highly skewed or contains outliers, the median may be a more appropriate measure of central tendency.      Two datasets with the same mean must have the same standard deviation.    This is not true. Datasets with the same mean can have vastly different spread or variability, resulting in different standard deviations.      Understanding variability is essential for making appropriate inferences from data.    Correct! Acknowledging and accounting for variability is a fundamental aspect of statistical thinking and is crucial for proper data interpretation.     "
},
{
  "id": "subsec-variability-in-analysis-3-1-1",
  "level": "2",
  "url": "sec-statistical-thinking.html#subsec-variability-in-analysis-3-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "When asking questions "
},
{
  "id": "subsec-variability-in-analysis-3-2-1",
  "level": "2",
  "url": "sec-statistical-thinking.html#subsec-variability-in-analysis-3-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "When considering data "
},
{
  "id": "subsec-variability-in-analysis-3-3-1",
  "level": "2",
  "url": "sec-statistical-thinking.html#subsec-variability-in-analysis-3-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "When analyzing data "
},
{
  "id": "subsec-variability-in-analysis-3-4-1",
  "level": "2",
  "url": "sec-statistical-thinking.html#subsec-variability-in-analysis-3-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "When interpreting results "
},
{
  "id": "example-variability-health",
  "level": "2",
  "url": "sec-statistical-thinking.html#example-variability-health",
  "type": "Example",
  "number": "46",
  "title": "Variability in Community Health Data.",
  "body": " Variability in Community Health Data   In our Community Health dataset, we might observe that:    Asthma rates vary considerably across neighborhoods (spatial variability)    Air quality measurements fluctuate seasonally (temporal variability)    The relationship between green space and obesity is stronger in some demographic groups than others (variability in relationships)    Some health metrics have greater measurement uncertainty than others (variability due to measurement)    Acknowledging these sources of variability helps us avoid oversimplified conclusions and recognize the complexity of environmental health relationships.   "
},
{
  "id": "activity-explore-variability",
  "level": "2",
  "url": "sec-statistical-thinking.html#activity-explore-variability",
  "type": "Activity",
  "number": "13",
  "title": "Exploring Variability in Your Dataset.",
  "body": " Exploring Variability in Your Dataset   In this activity, you'll explore sources of variability in your chosen dataset.     Identify at least three variables in your dataset and use CODAP to create visualizations showing their distributions.      For each variable, describe the pattern of variability you observe. Is the distribution symmetric, skewed, uniform, bimodal, or something else?      Consider what might cause the variability you're seeing. Is it natural variation in the phenomenon, differences between groups, measurement issues, or something else?    "
},
{
  "id": "matching-variability-measures",
  "level": "2",
  "url": "sec-statistical-thinking.html#matching-variability-measures",
  "type": "Checkpoint",
  "number": "47",
  "title": "Measures of Variability.",
  "body": " Measures of Variability   Match each statistical measure with the aspect of variability it best represents.     Standard deviation  Average distance of values from the mean    Interquartile range (IQR)  Spread of the middle 50% of the data    Range  Difference between the maximum and minimum values    Variance  Average of squared deviations from the mean    Coefficient of variation  Standard deviation relative to the mean    "
},
{
  "id": "sec-ethics-spotlight-sampling",
  "level": "1",
  "url": "sec-ethics-spotlight-sampling.html",
  "type": "Section",
  "number": "",
  "title": "Ethics Spotlight: Representation in Data",
  "body": " Ethics Spotlight: Representation in Data  As we plan our data investigations, it's crucial to consider who is represented in our data and who might be missing or underrepresented.  Key ethical considerations regarding representation include:    Selection bias : Does our data systematically exclude certain groups?    Sampling fairness : Does our sample adequately represent diverse populations?    Historical exclusion : Are we working with data that reflects historical patterns of exclusion?    Appropriate categorization : Do our categories respect how people identify themselves?    Contextual interpretation : Are we considering social and historical context when interpreting group differences?     Representation in Community Health Data   In our Community Health dataset, we might need to consider:    Whether health surveys reached residents who don't speak English    If environmental monitoring stations are distributed equitably across neighborhoods    Whether certain communities have historically been excluded from public health research    If the neighborhood boundaries used in our analysis reflect meaningful community divisions    How to interpret health disparities without reinforcing harmful stereotypes       Data Representation Scenarios   For each scenario, identify the primary ethical concern related to representation in data.     Scenario 1: A city collects feedback about public services through an online survey that requires a smartphone and internet access.  What is the primary ethical concern in this scenario?    a . Selection bias    b . Privacy violation    c . Inappropriate categorization    d . Excessive data collection    Scenario 2: A medical research study examines health outcomes using patient data from university hospitals in affluent neighborhoods.  What is the primary ethical concern in this scenario?    a . Inappropriate categorization    b . Sampling fairness    c . Data security    d . Transparency     Consider who might be included or excluded from each data collection approach, and how that might affect the conclusions drawn from the data.             Scenario 1: The primary ethical concern is Selection bias.  This method systematically excludes residents without smartphones or internet access, creating selection bias that likely underrepresents lower-income or elderly populations.  Scenario 2: The primary ethical concern is Sampling fairness.  The sample is biased toward patients who have access to university hospitals in affluent areas, potentially missing health patterns in underserved communities.         Evaluating Representation in Your Dataset   In this activity, you'll critically examine representation issues in your chosen dataset.     Identify at least three ways in which your dataset might not fully represent the population you're interested in studying.      Consider how these representation issues might affect the conclusions you can draw from your analysis.      Propose at least two strategies for acknowledging or addressing these representation issues in your investigation.     "
},
{
  "id": "sec-ethics-spotlight-sampling-4-1-1",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#sec-ethics-spotlight-sampling-4-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Selection bias "
},
{
  "id": "sec-ethics-spotlight-sampling-4-2-1",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#sec-ethics-spotlight-sampling-4-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Sampling fairness "
},
{
  "id": "sec-ethics-spotlight-sampling-4-3-1",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#sec-ethics-spotlight-sampling-4-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Historical exclusion "
},
{
  "id": "sec-ethics-spotlight-sampling-4-4-1",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#sec-ethics-spotlight-sampling-4-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Appropriate categorization "
},
{
  "id": "sec-ethics-spotlight-sampling-4-5-1",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#sec-ethics-spotlight-sampling-4-5-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Contextual interpretation "
},
{
  "id": "example-ethics-representation",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#example-ethics-representation",
  "type": "Example",
  "number": "48",
  "title": "Representation in Community Health Data.",
  "body": " Representation in Community Health Data   In our Community Health dataset, we might need to consider:    Whether health surveys reached residents who don't speak English    If environmental monitoring stations are distributed equitably across neighborhoods    Whether certain communities have historically been excluded from public health research    If the neighborhood boundaries used in our analysis reflect meaningful community divisions    How to interpret health disparities without reinforcing harmful stereotypes     "
},
{
  "id": "exercise-ethics-scenarios",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#exercise-ethics-scenarios",
  "type": "Checkpoint",
  "number": "49",
  "title": "Data Representation Scenarios.",
  "body": " Data Representation Scenarios   For each scenario, identify the primary ethical concern related to representation in data.     Scenario 1: A city collects feedback about public services through an online survey that requires a smartphone and internet access.  What is the primary ethical concern in this scenario?    a . Selection bias    b . Privacy violation    c . Inappropriate categorization    d . Excessive data collection    Scenario 2: A medical research study examines health outcomes using patient data from university hospitals in affluent neighborhoods.  What is the primary ethical concern in this scenario?    a . Inappropriate categorization    b . Sampling fairness    c . Data security    d . Transparency     Consider who might be included or excluded from each data collection approach, and how that might affect the conclusions drawn from the data.             Scenario 1: The primary ethical concern is Selection bias.  This method systematically excludes residents without smartphones or internet access, creating selection bias that likely underrepresents lower-income or elderly populations.  Scenario 2: The primary ethical concern is Sampling fairness.  The sample is biased toward patients who have access to university hospitals in affluent areas, potentially missing health patterns in underserved communities.       "
},
{
  "id": "activity-ethics-representation",
  "level": "2",
  "url": "sec-ethics-spotlight-sampling.html#activity-ethics-representation",
  "type": "Activity",
  "number": "14",
  "title": "Evaluating Representation in Your Dataset.",
  "body": " Evaluating Representation in Your Dataset   In this activity, you'll critically examine representation issues in your chosen dataset.     Identify at least three ways in which your dataset might not fully represent the population you're interested in studying.      Consider how these representation issues might affect the conclusions you can draw from your analysis.      Propose at least two strategies for acknowledging or addressing these representation issues in your investigation.    "
},
{
  "id": "sec-unit2-summary",
  "level": "1",
  "url": "sec-unit2-summary.html",
  "type": "Section",
  "number": "",
  "title": "Unit 2 Summary",
  "body": " Unit 2 Summary  In this unit, we've explored:    The data investigation framework developed by Hollylynne Lee and colleagues, which provides a structured approach to data analysis    How to formulate effective statistical questions that anticipate variability and can be answered with data    The components of a comprehensive investigation plan, including research questions, data requirements, analysis approaches, and potential challenges    Fundamental concepts of statistical thinking, particularly the importance of understanding and accounting for variability    Ethical considerations regarding representation in data and how they affect the conclusions we can draw    By the end of this unit, you should have a clear investigation plan for your project dataset, including well-formulated statistical questions and a strategy for analysis. This plan will guide your work in the upcoming units as we dive deeper into data moves and visualization techniques.   Unit 2 Reflection   Take some time to reflect on what you've learned in this unit:    How has the data investigation framework changed your approach to analyzing data?    What was most challenging about formulating effective statistical questions?    How does understanding variability influence the way you think about data?    What aspects of your investigation plan are you most confident about, and which might need refinement as you proceed?       Unit 2 Review   Which of the following BEST describes the main purpose of the data investigation framework?      To provide a rigid, linear sequence of steps that must be followed in every data analysis    The framework is not meant to be rigid or strictly linear. Real investigations often involve cycling back through earlier phases as new insights emerge.      To replace critical thinking with standardized procedures for data analysis    The framework is designed to enhance critical thinking, not replace it. It provides a structure while still requiring judgment and creativity.      To provide a structured approach that ensures all important aspects of data investigation are considered    Correct! The framework serves as a guide to help ensure that important elements like question formulation, data quality assessment, appropriate analysis, and careful interpretation are all addressed.      To automate the process of analyzing data so that minimal human intervention is required    The framework does not automate analysis; it provides a conceptual structure for human investigators to follow while still requiring substantial judgment and expertise.      "
},
{
  "id": "exercise-unit2-reflection",
  "level": "2",
  "url": "sec-unit2-summary.html#exercise-unit2-reflection",
  "type": "Checkpoint",
  "number": "50",
  "title": "Unit 2 Reflection.",
  "body": " Unit 2 Reflection   Take some time to reflect on what you've learned in this unit:    How has the data investigation framework changed your approach to analyzing data?    What was most challenging about formulating effective statistical questions?    How does understanding variability influence the way you think about data?    What aspects of your investigation plan are you most confident about, and which might need refinement as you proceed?     "
},
{
  "id": "mc-unit2-review",
  "level": "2",
  "url": "sec-unit2-summary.html#mc-unit2-review",
  "type": "Checkpoint",
  "number": "51",
  "title": "Unit 2 Review.",
  "body": " Unit 2 Review   Which of the following BEST describes the main purpose of the data investigation framework?      To provide a rigid, linear sequence of steps that must be followed in every data analysis    The framework is not meant to be rigid or strictly linear. Real investigations often involve cycling back through earlier phases as new insights emerge.      To replace critical thinking with standardized procedures for data analysis    The framework is designed to enhance critical thinking, not replace it. It provides a structure while still requiring judgment and creativity.      To provide a structured approach that ensures all important aspects of data investigation are considered    Correct! The framework serves as a guide to help ensure that important elements like question formulation, data quality assessment, appropriate analysis, and careful interpretation are all addressed.      To automate the process of analyzing data so that minimal human intervention is required    The framework does not automate analysis; it provides a conceptual structure for human investigators to follow while still requiring substantial judgment and expertise.     "
},
{
  "id": "sec-data-cleaning",
  "level": "1",
  "url": "sec-data-cleaning.html",
  "type": "Section",
  "number": "",
  "title": "Data Cleaning and Organization",
  "body": " Data Cleaning and Organization   Why Data Cleaning Matters  Data cleaning is often the most time-consuming part of data science work—and for good reason. The quality of your cleaning directly affects the validity of your results.   \"Garbage in, garbage out\" is a fundamental principle in data science. No amount of sophisticated analysis can compensate for poor-quality data.   Common data quality issues include:    Missing values : Data points that weren't recorded or were lost    Inconsistent formats : The same information recorded in different ways (e.g., \"NY\" vs. \"New York\")    Duplicate records : The same observation recorded multiple times    Outliers : Extreme values that may represent errors or unusual cases    Structural issues : Data not organized according to tidy data principles    Coding errors : Incorrect values due to human or system errors     Identifying Data Quality Issues   Which of the following would NOT typically be considered a data quality issue requiring cleaning?      A temperature column contains some values recorded in Celsius and others in Fahrenheit.    This is a data quality issue. Inconsistent units need to be standardized before analysis.      Several cells in a spreadsheet contain \"#N\/A\" or \"NULL\" values.    Missing values represented as \"#N\/A\" or \"NULL\" are data quality issues that need to be addressed.      A dataset shows that air pollution levels are higher in urban areas compared to rural areas.    Correct! This is an actual finding from the data rather than a quality issue. It represents a pattern or relationship that might emerge after proper cleaning and analysis.      Dates are stored in different formats such as \"01\/05\/2023\", \"Jan 5, 2023\", and \"2023-01-05\".    Inconsistent date formats are a data quality issue that needs to be standardized.      Data cleaning is both a science and an art. While there are standard approaches for handling common issues, every dataset presents unique challenges that require careful judgment.    Handling Missing Values  Missing values are perhaps the most common data quality issue. Before deciding how to handle them, it's important to understand why the data might be missing:    Missing Completely at Random (MCAR)  The missing values occur purely by chance, with no relationship to any variables in the dataset.    Missing at Random (MAR)  The probability of missing values depends on observed data but not on the missing values themselves.    Missing Not at Random (MNAR)  The missing values are related to the values themselves (e.g., people with high incomes might be less likely to report their income).    There are several common approaches to handling missing values:    Complete Case Analysis (Listwise Deletion)  Remove all rows with any missing values. Simple but can substantially reduce your dataset and introduce bias.    Single Imputation  Replace missing values with a calculated value like the mean, median, or mode. Straightforward but doesn't account for uncertainty.    Multiple Imputation  Generate multiple plausible values for each missing value, reflecting uncertainty. More complex but statistically sounder.    Indicator Variables  Create a new variable that flags where values were missing, then use imputation. Preserves information about missingness patterns.     Patterns of Missing Values   A visualization showing different patterns of missing values in a dataset: scattered randomly, entire columns missing, or specific combinations of variables missing together.    The appropriate approach depends on:    The pattern and mechanism of missingness    The proportion of missing values    The importance of the variable to your analysis    The size of your dataset     Missing Values in Community Health Data   In our Community Health dataset, we might encounter these missing value situations:    A few neighborhoods have missing air quality index values because monitoring stations were temporarily offline (likely MCAR).    Asthma rates are more likely to be missing in lower-income neighborhoods due to less comprehensive health reporting systems (MAR).    Obesity rates might be missing in neighborhoods where they are particularly high due to stigma-related underreporting (MNAR).    Different approaches might be appropriate for each situation:    For the missing air quality values, we might interpolate values from nearby stations.    For missing asthma rates, we might use multiple imputation based on other health and socioeconomic variables.    For the obesity data, we should acknowledge the potential bias and perhaps use an indicator variable approach to flag where data was missing.       Steps for Handling Missing Values   Arrange the following steps in a logical order for handling missing values in a dataset.     Identify variables with missing values and calculate the proportion of missingness.     Immediately delete all rows with any missing values.    Examine patterns of missingness to understand potential mechanisms (MCAR, MAR, MNAR).    Replace all missing values with zero.     Consider the impact of missingness on your specific research questions.    Select appropriate techniques for handling missing values based on the pattern, mechanism, and research goals.    Implement the chosen techniques and document your approach.    Ignore missing values since they don't affect analysis results.     Before deciding how to handle missing values, you need to understand the extent and pattern of missingness in your data.     Missing Values in CODAP   In this activity, you'll practice working with missing values in CODAP.     Open the Community Health dataset in CODAP (or your own project dataset).      Identify variables with missing values. CODAP typically shows these as empty cells. Count how many missing values exist for each variable.      Create a calculated attribute using the formula if(isNaN(originalAttribute), replacementValue, originalAttribute) to replace missing values in a numerical column with the mean or median of that column.      Create visualizations comparing the distribution of a variable before and after imputing missing values. How does the imputation affect the distribution?       Dealing with Outliers  Outliers are extreme values that differ significantly from other observations in the dataset. Not all outliers are errors—some represent genuine extreme cases that are important to understand.    An outlier is an observation that falls far outside the typical range of values in a dataset, often defined statistically as values more than 1.5 interquartile ranges below the first quartile or above the third quartile.    When encountering outliers, consider these questions:    Is the outlier a genuine observation or a data entry error?    If it's genuine, does it represent an important case that should be included in analysis?    How much influence does the outlier have on your statistics and visualizations?    What is the most appropriate way to handle this outlier given your research questions?    Common approaches to handling outliers include:    Retain  Keep outliers in the dataset, especially if they represent valid and important observations.    Remove  Delete outliers if they are clearly errors or if they unduly influence your analysis.    Transform  Apply mathematical transformations (like logarithmic) to reduce the influence of extreme values.    Cap  Set a maximum threshold and replace values exceeding it (winsorizing).    Analyze Separately  Conduct analyses both with and without outliers to understand their influence.     Outliers in Community Health Data   In our Community Health dataset, we might encounter several types of outliers:    A neighborhood shows an air quality index ten times higher than any other neighborhood, which investigation reveals was due to a data entry error (decimal point misplaced). This should be corrected.    One neighborhood has an unusually high asthma rate that is verified as accurate and corresponds to its proximity to a major industrial facility. This outlier should be retained as it represents an important case.    Income distribution across neighborhoods is highly skewed, with a few very wealthy areas. A log transformation might be appropriate for some analyses to better visualize relationships.       Detecting Outliers with Box Plots   A box plot showing the distribution of values with outliers marked as individual points beyond the whiskers.     Outlier Handling Approaches   Which approach to handling outliers would be MOST appropriate in this situation: You're analyzing neighborhood crime rates, and one neighborhood has a rate five times higher than the next highest. Upon investigation, you confirm this is accurate data for a neighborhood with unique circumstances.      Remove the neighborhood from your dataset to prevent it from skewing your statistical results.    This would not be appropriate since the outlier represents a genuine and potentially important case. Removing it would eliminate valuable information about an actual neighborhood.      Replace the crime rate value with the mean of all other neighborhoods.    Replacing a confirmed accurate value with the mean would discard valid information and misrepresent the actual situation in that neighborhood.      Retain the outlier but conduct analyses both with and without it to understand its influence on your results.    Correct! This approach preserves the valid data point while allowing you to assess its impact on your overall results. This provides a more complete understanding of neighborhood crime patterns.      Cap the value at three times the next highest rate to reduce its influence.    Capping a confirmed accurate value would artificially alter real data. Since the extreme value is valid, artificially reducing it would misrepresent the actual situation.       Outlier Detection in CODAP   In this activity, you'll practice identifying and examining outliers in CODAP.     Open your dataset in CODAP and create box plots for at least three numerical variables.      Identify potential outliers in each variable (points outside the whiskers of the box plot).      For each identified outlier, examine the complete record (row) to see if you can determine why it might be unusual. Is it likely an error or a genuine extreme case?      Create scatter plots showing relationships between variables, and identify any points that appear to be outliers in these relationships (even if they're not outliers in individual variables).       Renaming and Restructuring Data  Beyond fixing errors, data cleaning often involves restructuring the dataset to make it more suitable for analysis. This might include:    Renaming variables for clarity and consistency    Recoding values to standardize categories (e.g., \"M\" and \"Male\" to a single code)    Creating new variables based on existing ones    Reshaping data between wide and long formats    Merging datasets to combine information from different sources    A particularly important restructuring is ensuring your data follows the principles of tidy data:    Each variable forms a column    Each observation forms a row    Each type of observational unit forms a table     Restructuring Community Health Data   For our Community Health dataset, we might need to:    Rename variables from cryptic codes like \"AQI_AVG\" to clearer names like \"Average_Air_Quality_Index\"    Standardize inconsistent neighborhood classifications (e.g., \"Downtown\", \"Central Business District\", \"CBD\" all referring to the same area)    Create a new variable categorizing air quality as \"Good\", \"Moderate\", or \"Poor\" based on numerical AQI values    Convert data from a wide format (different health metrics in separate columns) to a long format (a single \"Health_Metric\" column with a \"Value\" column) for certain analyses       Data Restructuring Operations   Match each data restructuring operation with its most appropriate use case.     Converting from wide to long format  Preparing data where one observation is measured across multiple time points for time-series visualization    Recoding categorical values  Standardizing inconsistent entries like \"F\", \"female\", and \"fem\" to a single value    Creating calculated variables  Deriving Body Mass Index (BMI) from height and weight measurements    Merging datasets  Combining neighborhood health data with separate census demographic data using a common ID    Renaming variables  Changing cryptic column names like \"var001\" to descriptive names like \"annual_income\"      Restructuring Data in CODAP   In this activity, you'll practice renaming and restructuring data in CODAP.     In your dataset, identify at least three variables that could benefit from clearer, more descriptive names. Rename these attributes in CODAP by right-clicking on the column header.      Find a numerical variable that would be useful to categorize. Use CODAP's calculator to create a new attribute that categorizes values (e.g., creating \"Income_Level\" with values like \"Low\", \"Medium\", and \"High\" based on numeric income).      Create at least one calculated attribute that performs a mathematical operation on existing variables (e.g., a ratio, percentage, or unit conversion).      "
},
{
  "id": "subsec-importance-cleaning-5-1-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-importance-cleaning-5-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Missing values "
},
{
  "id": "subsec-importance-cleaning-5-2-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-importance-cleaning-5-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Inconsistent formats "
},
{
  "id": "subsec-importance-cleaning-5-3-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-importance-cleaning-5-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Duplicate records "
},
{
  "id": "subsec-importance-cleaning-5-4-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-importance-cleaning-5-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Outliers "
},
{
  "id": "subsec-importance-cleaning-5-5-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-importance-cleaning-5-5-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Structural issues "
},
{
  "id": "subsec-importance-cleaning-5-6-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-importance-cleaning-5-6-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Coding errors "
},
{
  "id": "mc-data-quality-issues",
  "level": "2",
  "url": "sec-data-cleaning.html#mc-data-quality-issues",
  "type": "Checkpoint",
  "number": "52",
  "title": "Identifying Data Quality Issues.",
  "body": " Identifying Data Quality Issues   Which of the following would NOT typically be considered a data quality issue requiring cleaning?      A temperature column contains some values recorded in Celsius and others in Fahrenheit.    This is a data quality issue. Inconsistent units need to be standardized before analysis.      Several cells in a spreadsheet contain \"#N\/A\" or \"NULL\" values.    Missing values represented as \"#N\/A\" or \"NULL\" are data quality issues that need to be addressed.      A dataset shows that air pollution levels are higher in urban areas compared to rural areas.    Correct! This is an actual finding from the data rather than a quality issue. It represents a pattern or relationship that might emerge after proper cleaning and analysis.      Dates are stored in different formats such as \"01\/05\/2023\", \"Jan 5, 2023\", and \"2023-01-05\".    Inconsistent date formats are a data quality issue that needs to be standardized.     "
},
{
  "id": "fig-missing-values",
  "level": "2",
  "url": "sec-data-cleaning.html#fig-missing-values",
  "type": "Figure",
  "number": "53",
  "title": "",
  "body": " Patterns of Missing Values   A visualization showing different patterns of missing values in a dataset: scattered randomly, entire columns missing, or specific combinations of variables missing together.   "
},
{
  "id": "example-missing-health",
  "level": "2",
  "url": "sec-data-cleaning.html#example-missing-health",
  "type": "Example",
  "number": "54",
  "title": "Missing Values in Community Health Data.",
  "body": " Missing Values in Community Health Data   In our Community Health dataset, we might encounter these missing value situations:    A few neighborhoods have missing air quality index values because monitoring stations were temporarily offline (likely MCAR).    Asthma rates are more likely to be missing in lower-income neighborhoods due to less comprehensive health reporting systems (MAR).    Obesity rates might be missing in neighborhoods where they are particularly high due to stigma-related underreporting (MNAR).    Different approaches might be appropriate for each situation:    For the missing air quality values, we might interpolate values from nearby stations.    For missing asthma rates, we might use multiple imputation based on other health and socioeconomic variables.    For the obesity data, we should acknowledge the potential bias and perhaps use an indicator variable approach to flag where data was missing.     "
},
{
  "id": "parsons-missing-values",
  "level": "2",
  "url": "sec-data-cleaning.html#parsons-missing-values",
  "type": "Checkpoint",
  "number": "55",
  "title": "Steps for Handling Missing Values.",
  "body": " Steps for Handling Missing Values   Arrange the following steps in a logical order for handling missing values in a dataset.     Identify variables with missing values and calculate the proportion of missingness.     Immediately delete all rows with any missing values.    Examine patterns of missingness to understand potential mechanisms (MCAR, MAR, MNAR).    Replace all missing values with zero.     Consider the impact of missingness on your specific research questions.    Select appropriate techniques for handling missing values based on the pattern, mechanism, and research goals.    Implement the chosen techniques and document your approach.    Ignore missing values since they don't affect analysis results.     Before deciding how to handle missing values, you need to understand the extent and pattern of missingness in your data.   "
},
{
  "id": "activity-missing-values",
  "level": "2",
  "url": "sec-data-cleaning.html#activity-missing-values",
  "type": "Activity",
  "number": "15",
  "title": "Missing Values in CODAP.",
  "body": " Missing Values in CODAP   In this activity, you'll practice working with missing values in CODAP.     Open the Community Health dataset in CODAP (or your own project dataset).      Identify variables with missing values. CODAP typically shows these as empty cells. Count how many missing values exist for each variable.      Create a calculated attribute using the formula if(isNaN(originalAttribute), replacementValue, originalAttribute) to replace missing values in a numerical column with the mean or median of that column.      Create visualizations comparing the distribution of a variable before and after imputing missing values. How does the imputation affect the distribution?    "
},
{
  "id": "def-outlier",
  "level": "2",
  "url": "sec-data-cleaning.html#def-outlier",
  "type": "Definition",
  "number": "56",
  "title": "",
  "body": "  An outlier is an observation that falls far outside the typical range of values in a dataset, often defined statistically as values more than 1.5 interquartile ranges below the first quartile or above the third quartile.   "
},
{
  "id": "example-outliers-health",
  "level": "2",
  "url": "sec-data-cleaning.html#example-outliers-health",
  "type": "Example",
  "number": "57",
  "title": "Outliers in Community Health Data.",
  "body": " Outliers in Community Health Data   In our Community Health dataset, we might encounter several types of outliers:    A neighborhood shows an air quality index ten times higher than any other neighborhood, which investigation reveals was due to a data entry error (decimal point misplaced). This should be corrected.    One neighborhood has an unusually high asthma rate that is verified as accurate and corresponds to its proximity to a major industrial facility. This outlier should be retained as it represents an important case.    Income distribution across neighborhoods is highly skewed, with a few very wealthy areas. A log transformation might be appropriate for some analyses to better visualize relationships.     "
},
{
  "id": "fig-outlier-detection",
  "level": "2",
  "url": "sec-data-cleaning.html#fig-outlier-detection",
  "type": "Figure",
  "number": "58",
  "title": "",
  "body": " Detecting Outliers with Box Plots   A box plot showing the distribution of values with outliers marked as individual points beyond the whiskers.   "
},
{
  "id": "mc-outlier-handling",
  "level": "2",
  "url": "sec-data-cleaning.html#mc-outlier-handling",
  "type": "Checkpoint",
  "number": "59",
  "title": "Outlier Handling Approaches.",
  "body": " Outlier Handling Approaches   Which approach to handling outliers would be MOST appropriate in this situation: You're analyzing neighborhood crime rates, and one neighborhood has a rate five times higher than the next highest. Upon investigation, you confirm this is accurate data for a neighborhood with unique circumstances.      Remove the neighborhood from your dataset to prevent it from skewing your statistical results.    This would not be appropriate since the outlier represents a genuine and potentially important case. Removing it would eliminate valuable information about an actual neighborhood.      Replace the crime rate value with the mean of all other neighborhoods.    Replacing a confirmed accurate value with the mean would discard valid information and misrepresent the actual situation in that neighborhood.      Retain the outlier but conduct analyses both with and without it to understand its influence on your results.    Correct! This approach preserves the valid data point while allowing you to assess its impact on your overall results. This provides a more complete understanding of neighborhood crime patterns.      Cap the value at three times the next highest rate to reduce its influence.    Capping a confirmed accurate value would artificially alter real data. Since the extreme value is valid, artificially reducing it would misrepresent the actual situation.     "
},
{
  "id": "activity-outlier-detection",
  "level": "2",
  "url": "sec-data-cleaning.html#activity-outlier-detection",
  "type": "Activity",
  "number": "16",
  "title": "Outlier Detection in CODAP.",
  "body": " Outlier Detection in CODAP   In this activity, you'll practice identifying and examining outliers in CODAP.     Open your dataset in CODAP and create box plots for at least three numerical variables.      Identify potential outliers in each variable (points outside the whiskers of the box plot).      For each identified outlier, examine the complete record (row) to see if you can determine why it might be unusual. Is it likely an error or a genuine extreme case?      Create scatter plots showing relationships between variables, and identify any points that appear to be outliers in these relationships (even if they're not outliers in individual variables).    "
},
{
  "id": "subsec-restructuring-3-1-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-restructuring-3-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Renaming variables "
},
{
  "id": "subsec-restructuring-3-2-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-restructuring-3-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Recoding values "
},
{
  "id": "subsec-restructuring-3-3-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-restructuring-3-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Creating new variables "
},
{
  "id": "subsec-restructuring-3-4-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-restructuring-3-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Reshaping data "
},
{
  "id": "subsec-restructuring-3-5-1",
  "level": "2",
  "url": "sec-data-cleaning.html#subsec-restructuring-3-5-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Merging datasets "
},
{
  "id": "example-restructuring-health",
  "level": "2",
  "url": "sec-data-cleaning.html#example-restructuring-health",
  "type": "Example",
  "number": "60",
  "title": "Restructuring Community Health Data.",
  "body": " Restructuring Community Health Data   For our Community Health dataset, we might need to:    Rename variables from cryptic codes like \"AQI_AVG\" to clearer names like \"Average_Air_Quality_Index\"    Standardize inconsistent neighborhood classifications (e.g., \"Downtown\", \"Central Business District\", \"CBD\" all referring to the same area)    Create a new variable categorizing air quality as \"Good\", \"Moderate\", or \"Poor\" based on numerical AQI values    Convert data from a wide format (different health metrics in separate columns) to a long format (a single \"Health_Metric\" column with a \"Value\" column) for certain analyses     "
},
{
  "id": "matching-restructuring",
  "level": "2",
  "url": "sec-data-cleaning.html#matching-restructuring",
  "type": "Checkpoint",
  "number": "61",
  "title": "Data Restructuring Operations.",
  "body": " Data Restructuring Operations   Match each data restructuring operation with its most appropriate use case.     Converting from wide to long format  Preparing data where one observation is measured across multiple time points for time-series visualization    Recoding categorical values  Standardizing inconsistent entries like \"F\", \"female\", and \"fem\" to a single value    Creating calculated variables  Deriving Body Mass Index (BMI) from height and weight measurements    Merging datasets  Combining neighborhood health data with separate census demographic data using a common ID    Renaming variables  Changing cryptic column names like \"var001\" to descriptive names like \"annual_income\"    "
},
{
  "id": "activity-data-restructuring",
  "level": "2",
  "url": "sec-data-cleaning.html#activity-data-restructuring",
  "type": "Activity",
  "number": "17",
  "title": "Restructuring Data in CODAP.",
  "body": " Restructuring Data in CODAP   In this activity, you'll practice renaming and restructuring data in CODAP.     In your dataset, identify at least three variables that could benefit from clearer, more descriptive names. Rename these attributes in CODAP by right-clicking on the column header.      Find a numerical variable that would be useful to categorize. Use CODAP's calculator to create a new attribute that categorizes values (e.g., creating \"Income_Level\" with values like \"Low\", \"Medium\", and \"High\" based on numeric income).      Create at least one calculated attribute that performs a mathematical operation on existing variables (e.g., a ratio, percentage, or unit conversion).    "
},
{
  "id": "sec-filtering-subsetting",
  "level": "1",
  "url": "sec-filtering-subsetting.html",
  "type": "Section",
  "number": "",
  "title": "Filtering and Subsetting",
  "body": " Filtering and Subsetting   The Purpose of Filtering  Rarely do we analyze an entire dataset at once. More often, we focus on specific subsets of the data that are relevant to particular questions. Filtering and subsetting allow us to:    Focus on specific groups or conditions of interest    Compare different subsets to identify patterns and differences    Remove irrelevant data that might obscure important relationships    Create more manageable subsets for specialized analyses    Test relationships under different conditions    Effective filtering requires clear criteria and an understanding of how the filtering might affect your analysis.   Filtering in Community Health Analysis   In our Community Health dataset, we might apply these filters:    Focus only on neighborhoods with complete data across all health metrics    Compare high-income versus low-income neighborhoods (using median household income)    Examine only neighborhoods with poor air quality to understand health patterns in most affected areas    Create separate analyses for different regions of the city (north, south, east, west)    Filter out neighborhoods undergoing major redevelopment that might skew environmental measurements       Purposes of Filtering   Which of the following is NOT a valid reason to filter or subset data?      To compare outcomes between different demographic groups    This is a valid reason for filtering—creating subsets based on demographic variables can reveal important differences between groups.      To focus analysis on the most recent time period in a longitudinal dataset    This is a valid reason for filtering—focusing on the most recent data can provide insights into current conditions.      To remove data points that contradict your hypothesis    Correct! This is NOT a valid reason for filtering. Removing data merely because it contradicts your hypothesis introduces bias and violates principles of scientific integrity.      To create a more manageable dataset for complex computational methods    This is a valid reason for filtering—some analyses may require smaller datasets due to computational constraints, as long as the subsetting is done in a principled way.        Filtering Methods in CODAP  CODAP provides several ways to filter data:    Selection from Visualizations  Clicking on points in a graph or cells in a table selects those cases. You can then hide unselected cases or create a new collection with only selected cases.    Filter Using Formulas  Create a filter using a formula like income > 50000 to show only cases meeting that condition.    Creating Subsets  Create a new dataset containing only filtered data, preserving the original dataset.    Hierarchical Organization  Organize data into hierarchical collections, allowing analysis at different levels (e.g., cities → neighborhoods → households).     Filtering Data in CODAP   Screenshot showing CODAP's filtering interface with formula input and filtered results.    When filtering data, it's important to:    Document your filtering criteria clearly    Consider how the filter might affect the representativeness of your data    Be aware of how sample size reduction might impact statistical analyses    Check whether your filtered data still addresses your research questions     Filtering Data in CODAP   In this activity, you'll practice filtering data in CODAP using different methods.     Open your dataset in CODAP and create a scatter plot using two numerical variables.      Use selection to highlight a cluster of points, then create a new collection containing only these selected cases.      Create a filter using the formula editor to show only cases meeting specific criteria (e.g., values above a threshold or matching a category).      Compare summary statistics of your original dataset and filtered subset. How do measures like mean, median, and standard deviation change when you apply your filter?      Translating Filter Statements   For each of the following filtering objectives, select the CODAP filter formula that would accomplish it.     Show only neighborhoods with both above-average income and above-average green space.      Income > AverageIncome OR GreenSpace > AverageGreenSpace    This formula uses OR, which would show neighborhoods with either above-average income OR above-average green space, not necessarily both.      Income > AverageIncome AND GreenSpace > AverageGreenSpace    Correct! This formula uses AND to require that both conditions (above-average income and above-average green space) must be true for a neighborhood to be included in the filter.      Income = AverageIncome AND GreenSpace = AverageGreenSpace    This formula would only show neighborhoods with exactly average income and exactly average green space, not those above average.      Income > AverageIncome    This formula only checks for above-average income and doesn't consider green space at all.        Show neighborhoods that are either in the North region or have low pollution levels.      Region = \"North\" AND PollutionLevel = \"Low\"    This formula uses AND, which would only show neighborhoods that are both in the North region AND have low pollution levels, not either condition.      NOT(Region = \"North\") OR NOT(PollutionLevel = \"Low\")    This formula would show neighborhoods that are either NOT in the North region OR do NOT have low pollution levels, which is almost the opposite of what we want.      Region = \"North\" OR PollutionLevel = \"Low\"    Correct! This formula uses OR to include neighborhoods that meet either condition: being in the North region OR having low pollution levels.      Region != \"North\" AND PollutionLevel != \"Low\"    This formula would show only neighborhoods that are NOT in the North region AND do NOT have low pollution levels, which is the exact opposite of what we want.         Creating Meaningful Subsets  Beyond simple filtering, creating meaningful subsets often involves more complex criteria and a deeper understanding of your data. Effective subsetting strategies include:    Comparative Subsets  Create groups for comparison based on key variables (e.g., high vs. low exposure groups, different demographic categories).    Threshold-Based Subsets  Define groups based on meaningful thresholds like regulatory standards or clinical definitions.    Time-Based Subsets  Create groups based on time periods to study changes or compare before\/after scenarios.    Cluster-Based Subsets  Use patterns in the data itself to identify natural groupings.    Random Sampling  Create representative subsets through random sampling, particularly for very large datasets.     Meaningful Subsets in Community Health   For our Community Health dataset, meaningful subsets might include:    Income quantiles : Dividing neighborhoods into income quintiles (bottom 20%, 20-40%, etc.) to examine how health patterns vary across socioeconomic spectrum    Environmental risk categories : Grouping neighborhoods as \"high-risk\" or \"low-risk\" based on combined environmental factors    Geographic regions : Creating subsets based on meaningful geographic divisions like urban core, inner suburbs, outer suburbs    Health outcome groups : Identifying neighborhoods with multiple poor health outcomes versus those with generally good outcomes    Green space access : Comparing neighborhoods with high, medium, and low access to green spaces       Creating Meaningful Subsets   In this activity, you'll create and analyze meaningful subsets of your data.     Identify a key numerical variable in your dataset. Create a new categorical attribute that divides this variable into meaningful groups (e.g., low\/medium\/high, or quantiles).      Create visualizations comparing these groups across other variables. Look for patterns or differences between the groups.      Devise at least two different ways to create subsets from your data that might reveal interesting patterns. Implement these in CODAP and explore the results.      Write a brief summary of what you learned by examining different subsets of your data. Were there patterns that only became apparent when looking at specific subsets?      Subset Creation Approaches   A researcher is studying the relationship between exercise habits and health outcomes. Which subsetting approach would be MOST useful for comparing the effects of different exercise levels?      Random sampling to create smaller, more manageable datasets    While random sampling can be useful for creating manageable datasets from large ones, it doesn't specifically help with comparing different exercise levels.      Time-based subsetting to examine seasonal variations in exercise    While seasonal variations might be interesting, this approach doesn't directly address comparing different exercise levels and their health effects.      Threshold-based groups dividing participants into categories based on weekly exercise minutes (e.g., sedentary, moderately active, highly active)    Correct! Creating categorical groups based on meaningful exercise thresholds provides a clear way to compare health outcomes across different exercise levels.      Geographic subsetting to compare exercise habits across different regions    While regional comparisons might reveal interesting patterns in exercise habits, this approach focuses on geographic differences rather than directly comparing exercise levels and their health effects.       "
},
{
  "id": "example-filtering-health",
  "level": "2",
  "url": "sec-filtering-subsetting.html#example-filtering-health",
  "type": "Example",
  "number": "62",
  "title": "Filtering in Community Health Analysis.",
  "body": " Filtering in Community Health Analysis   In our Community Health dataset, we might apply these filters:    Focus only on neighborhoods with complete data across all health metrics    Compare high-income versus low-income neighborhoods (using median household income)    Examine only neighborhoods with poor air quality to understand health patterns in most affected areas    Create separate analyses for different regions of the city (north, south, east, west)    Filter out neighborhoods undergoing major redevelopment that might skew environmental measurements     "
},
{
  "id": "mc-filtering-purpose",
  "level": "2",
  "url": "sec-filtering-subsetting.html#mc-filtering-purpose",
  "type": "Checkpoint",
  "number": "63",
  "title": "Purposes of Filtering.",
  "body": " Purposes of Filtering   Which of the following is NOT a valid reason to filter or subset data?      To compare outcomes between different demographic groups    This is a valid reason for filtering—creating subsets based on demographic variables can reveal important differences between groups.      To focus analysis on the most recent time period in a longitudinal dataset    This is a valid reason for filtering—focusing on the most recent data can provide insights into current conditions.      To remove data points that contradict your hypothesis    Correct! This is NOT a valid reason for filtering. Removing data merely because it contradicts your hypothesis introduces bias and violates principles of scientific integrity.      To create a more manageable dataset for complex computational methods    This is a valid reason for filtering—some analyses may require smaller datasets due to computational constraints, as long as the subsetting is done in a principled way.     "
},
{
  "id": "fig-codap-filtering",
  "level": "2",
  "url": "sec-filtering-subsetting.html#fig-codap-filtering",
  "type": "Figure",
  "number": "64",
  "title": "",
  "body": " Filtering Data in CODAP   Screenshot showing CODAP's filtering interface with formula input and filtered results.   "
},
{
  "id": "activity-codap-filtering",
  "level": "2",
  "url": "sec-filtering-subsetting.html#activity-codap-filtering",
  "type": "Activity",
  "number": "18",
  "title": "Filtering Data in CODAP.",
  "body": " Filtering Data in CODAP   In this activity, you'll practice filtering data in CODAP using different methods.     Open your dataset in CODAP and create a scatter plot using two numerical variables.      Use selection to highlight a cluster of points, then create a new collection containing only these selected cases.      Create a filter using the formula editor to show only cases meeting specific criteria (e.g., values above a threshold or matching a category).      Compare summary statistics of your original dataset and filtered subset. How do measures like mean, median, and standard deviation change when you apply your filter?    "
},
{
  "id": "exercise-filtering-statements",
  "level": "2",
  "url": "sec-filtering-subsetting.html#exercise-filtering-statements",
  "type": "Checkpoint",
  "number": "65",
  "title": "Translating Filter Statements.",
  "body": " Translating Filter Statements   For each of the following filtering objectives, select the CODAP filter formula that would accomplish it.     Show only neighborhoods with both above-average income and above-average green space.      Income > AverageIncome OR GreenSpace > AverageGreenSpace    This formula uses OR, which would show neighborhoods with either above-average income OR above-average green space, not necessarily both.      Income > AverageIncome AND GreenSpace > AverageGreenSpace    Correct! This formula uses AND to require that both conditions (above-average income and above-average green space) must be true for a neighborhood to be included in the filter.      Income = AverageIncome AND GreenSpace = AverageGreenSpace    This formula would only show neighborhoods with exactly average income and exactly average green space, not those above average.      Income > AverageIncome    This formula only checks for above-average income and doesn't consider green space at all.        Show neighborhoods that are either in the North region or have low pollution levels.      Region = \"North\" AND PollutionLevel = \"Low\"    This formula uses AND, which would only show neighborhoods that are both in the North region AND have low pollution levels, not either condition.      NOT(Region = \"North\") OR NOT(PollutionLevel = \"Low\")    This formula would show neighborhoods that are either NOT in the North region OR do NOT have low pollution levels, which is almost the opposite of what we want.      Region = \"North\" OR PollutionLevel = \"Low\"    Correct! This formula uses OR to include neighborhoods that meet either condition: being in the North region OR having low pollution levels.      Region != \"North\" AND PollutionLevel != \"Low\"    This formula would show only neighborhoods that are NOT in the North region AND do NOT have low pollution levels, which is the exact opposite of what we want.      "
},
{
  "id": "example-subsets-health",
  "level": "2",
  "url": "sec-filtering-subsetting.html#example-subsets-health",
  "type": "Example",
  "number": "66",
  "title": "Meaningful Subsets in Community Health.",
  "body": " Meaningful Subsets in Community Health   For our Community Health dataset, meaningful subsets might include:    Income quantiles : Dividing neighborhoods into income quintiles (bottom 20%, 20-40%, etc.) to examine how health patterns vary across socioeconomic spectrum    Environmental risk categories : Grouping neighborhoods as \"high-risk\" or \"low-risk\" based on combined environmental factors    Geographic regions : Creating subsets based on meaningful geographic divisions like urban core, inner suburbs, outer suburbs    Health outcome groups : Identifying neighborhoods with multiple poor health outcomes versus those with generally good outcomes    Green space access : Comparing neighborhoods with high, medium, and low access to green spaces     "
},
{
  "id": "activity-create-subsets",
  "level": "2",
  "url": "sec-filtering-subsetting.html#activity-create-subsets",
  "type": "Activity",
  "number": "19",
  "title": "Creating Meaningful Subsets.",
  "body": " Creating Meaningful Subsets   In this activity, you'll create and analyze meaningful subsets of your data.     Identify a key numerical variable in your dataset. Create a new categorical attribute that divides this variable into meaningful groups (e.g., low\/medium\/high, or quantiles).      Create visualizations comparing these groups across other variables. Look for patterns or differences between the groups.      Devise at least two different ways to create subsets from your data that might reveal interesting patterns. Implement these in CODAP and explore the results.      Write a brief summary of what you learned by examining different subsets of your data. Were there patterns that only became apparent when looking at specific subsets?    "
},
{
  "id": "mc-subset-creation",
  "level": "2",
  "url": "sec-filtering-subsetting.html#mc-subset-creation",
  "type": "Checkpoint",
  "number": "67",
  "title": "Subset Creation Approaches.",
  "body": " Subset Creation Approaches   A researcher is studying the relationship between exercise habits and health outcomes. Which subsetting approach would be MOST useful for comparing the effects of different exercise levels?      Random sampling to create smaller, more manageable datasets    While random sampling can be useful for creating manageable datasets from large ones, it doesn't specifically help with comparing different exercise levels.      Time-based subsetting to examine seasonal variations in exercise    While seasonal variations might be interesting, this approach doesn't directly address comparing different exercise levels and their health effects.      Threshold-based groups dividing participants into categories based on weekly exercise minutes (e.g., sedentary, moderately active, highly active)    Correct! Creating categorical groups based on meaningful exercise thresholds provides a clear way to compare health outcomes across different exercise levels.      Geographic subsetting to compare exercise habits across different regions    While regional comparisons might reveal interesting patterns in exercise habits, this approach focuses on geographic differences rather than directly comparing exercise levels and their health effects.     "
},
{
  "id": "sec-ethics-spotlight-selection",
  "level": "1",
  "url": "sec-ethics-spotlight-selection.html",
  "type": "Section",
  "number": "",
  "title": "Ethics Spotlight: Selection Bias",
  "body": " Ethics Spotlight: Selection Bias  When filtering and subsetting data, we must be careful not to introduce selection bias—a distortion in our results due to the way we select cases for analysis.     Selection bias occurs when the data selected for analysis is not representative of the population about which conclusions are to be drawn, leading to systematic error in the findings.    Common forms of selection bias include:    Sampling Bias  The sample selected for study doesn't represent the population of interest.    Self-Selection Bias  Participants choose whether to participate, potentially introducing systematic differences between participants and non-participants.    Survival Bias  Analysis focuses only on cases that \"survived\" some process, ignoring those that didn't.    Exclusion Bias  Systematic exclusion of certain groups or cases due to methodological choices.    Confirmation Bias  Tendency to filter data in ways that confirm preexisting beliefs or hypotheses.     Selection Bias in Community Health Research   In our Community Health project, selection bias might occur if:    We exclude neighborhoods with missing data, which happen to be primarily lower-income areas    Health survey data includes only responses from residents who volunteered to participate    Air quality measurements are taken only during weekdays, missing weekend patterns    We focus our analysis only on neighborhoods with good outcomes to identify \"best practices\"    We filter data to include only cases that support our initial hypothesis about environmental factors    Each of these filtering or selection decisions could lead to conclusions that don't accurately represent the true relationships in the complete population.    To minimize selection bias when filtering data:    Document all filtering criteria and justify them based on substantive rather than convenient reasons    Consider how excluded cases differ from included ones    Perform sensitivity analysis by comparing results with different filtering criteria    Be transparent about the limitations of your filtered dataset    Actively look for potential sources of bias in your selection process     Identifying Selection Bias   Which scenario represents the clearest example of selection bias?      A researcher randomly selects 100 participants from a complete list of 1,000 eligible individuals.    This describes proper random sampling rather than selection bias, as every eligible individual had an equal chance of being selected.      After collecting data, a researcher finds that one variable has a non-normal distribution.    A non-normal distribution is not necessarily an indication of bias; many variables naturally have skewed or other non-normal distributions.      A study finds weak but statistically significant correlations between variables.    Finding weak correlations is not an indication of selection bias; it simply describes the strength of relationships in the data.      A survey about internet usage is conducted exclusively through online questionnaires.    Correct! This is a clear example of selection bias. By conducting the survey only online, the study systematically excludes people with limited or no internet access, who likely have different internet usage patterns than those who regularly use the internet.       Identifying Potential Selection Bias   In this activity, you'll examine potential selection bias in your own data analysis.     Review the filtering and subsetting operations you've performed on your dataset. For each filter, identify who or what is being excluded.      Consider how these exclusions might affect your conclusions. Are you systematically excluding certain types of cases?      Identify at least one potential source of selection bias in your data collection process (before you even received the dataset).      Propose strategies for addressing or minimizing the selection bias you've identified.     "
},
{
  "id": "def-selection-bias",
  "level": "2",
  "url": "sec-ethics-spotlight-selection.html#def-selection-bias",
  "type": "Definition",
  "number": "68",
  "title": "",
  "body": "   Selection bias occurs when the data selected for analysis is not representative of the population about which conclusions are to be drawn, leading to systematic error in the findings.   "
},
{
  "id": "example-selection-bias-health",
  "level": "2",
  "url": "sec-ethics-spotlight-selection.html#example-selection-bias-health",
  "type": "Example",
  "number": "69",
  "title": "Selection Bias in Community Health Research.",
  "body": " Selection Bias in Community Health Research   In our Community Health project, selection bias might occur if:    We exclude neighborhoods with missing data, which happen to be primarily lower-income areas    Health survey data includes only responses from residents who volunteered to participate    Air quality measurements are taken only during weekdays, missing weekend patterns    We focus our analysis only on neighborhoods with good outcomes to identify \"best practices\"    We filter data to include only cases that support our initial hypothesis about environmental factors    Each of these filtering or selection decisions could lead to conclusions that don't accurately represent the true relationships in the complete population.   "
},
{
  "id": "mc-selection-bias",
  "level": "2",
  "url": "sec-ethics-spotlight-selection.html#mc-selection-bias",
  "type": "Checkpoint",
  "number": "70",
  "title": "Identifying Selection Bias.",
  "body": " Identifying Selection Bias   Which scenario represents the clearest example of selection bias?      A researcher randomly selects 100 participants from a complete list of 1,000 eligible individuals.    This describes proper random sampling rather than selection bias, as every eligible individual had an equal chance of being selected.      After collecting data, a researcher finds that one variable has a non-normal distribution.    A non-normal distribution is not necessarily an indication of bias; many variables naturally have skewed or other non-normal distributions.      A study finds weak but statistically significant correlations between variables.    Finding weak correlations is not an indication of selection bias; it simply describes the strength of relationships in the data.      A survey about internet usage is conducted exclusively through online questionnaires.    Correct! This is a clear example of selection bias. By conducting the survey only online, the study systematically excludes people with limited or no internet access, who likely have different internet usage patterns than those who regularly use the internet.     "
},
{
  "id": "activity-selection-bias",
  "level": "2",
  "url": "sec-ethics-spotlight-selection.html#activity-selection-bias",
  "type": "Activity",
  "number": "20",
  "title": "Identifying Potential Selection Bias.",
  "body": " Identifying Potential Selection Bias   In this activity, you'll examine potential selection bias in your own data analysis.     Review the filtering and subsetting operations you've performed on your dataset. For each filter, identify who or what is being excluded.      Consider how these exclusions might affect your conclusions. Are you systematically excluding certain types of cases?      Identify at least one potential source of selection bias in your data collection process (before you even received the dataset).      Propose strategies for addressing or minimizing the selection bias you've identified.    "
},
{
  "id": "sec-summarizing-calculating",
  "level": "1",
  "url": "sec-summarizing-calculating.html",
  "type": "Section",
  "number": "",
  "title": "Summarizing, Calculating, and Grouping",
  "body": " Summarizing, Calculating, and Grouping   Essential Summary Statistics  Summary statistics condense complex datasets into manageable metrics that capture key aspects of the data. Common summary statistics include:    Measures of Central Tendency    Mean : The arithmetic average (sum divided by count)  Median : The middle value when data is ordered  Mode : The most frequently occurring value      Measures of Spread    Range : The difference between maximum and minimum values  Variance : The average squared deviation from the mean  Standard Deviation : The square root of variance  Interquartile Range (IQR) : The range of the middle 50% of values      Measures of Position    Percentiles : Values below which a given percentage of observations fall  Quartiles : Values that divide data into quarters  Z-scores : How many standard deviations a value is from the mean      Measures of Relationship    Correlation : The strength and direction of linear relationships  Covariance : How two variables vary together  Contingency tables : Counts of co-occurrences for categorical variables       Summary Statistics for Different Distributions   Three different distributions showing how the same mean and standard deviation can represent very different data patterns.    When choosing summary statistics, consider:    The type and scale of your data (categorical, ordinal, interval, ratio)    The shape of your distribution (symmetrical, skewed, multimodal)    The presence of outliers or extreme values    The specific aspects of the data you want to highlight     Choosing Appropriate Summary Statistics   For a highly skewed distribution of housing prices in a city, which measure of central tendency would be MOST appropriate to report?      Mean (average) price    The mean is highly influenced by extreme values and can be misleading for skewed distributions. In housing prices, a few very expensive properties can pull the mean upward, making it unrepresentative of typical prices.      Median price    Correct! The median is the most appropriate measure for skewed distributions like housing prices because it's not influenced by extreme values. It represents the middle value, giving a better sense of the \"typical\" home price.      Modal price (most common price)    While the mode can be useful for categorical data, it's generally less informative for continuous variables like housing prices, which may not have many exact repeated values.      Midrange (average of minimum and maximum prices)    The midrange is highly influenced by outliers and would be particularly problematic for a skewed distribution of housing prices, where the maximum value could be dramatically higher than most values.       Calculating Summary Statistics in CODAP   In this activity, you'll calculate and interpret summary statistics for your dataset.     Select at least three numerical variables from your dataset. For each variable, use CODAP to calculate:    Mean, median, and mode    Standard deviation and interquartile range    Minimum, maximum, and range        Create visualizations (histograms or box plots) for each variable and examine how the summary statistics relate to the distribution shape.      For each variable, determine which measure of central tendency (mean, median, or mode) best represents the \"typical\" value and explain why.      Calculate correlation coefficients between pairs of numerical variables. Identify the strongest positive and negative correlations in your dataset.       Creating Derived Variables  Often, the variables we need for analysis aren't directly present in our original dataset. Creating derived variables allows us to transform existing data into more meaningful measures.  Common types of derived variables include:    Mathematical Transformations    Logarithmic transformations to handle skewed data  Standardization (z-scores) to compare different scales  Unit conversions (e.g., meters to feet, Celsius to Fahrenheit)      Combinations of Variables    Ratios and proportions (e.g., debt-to-income ratio)  Indices combining multiple measures (e.g., air quality index)  Weighted averages (e.g., GPA calculation)      Categorical Derivations    Binning numerical variables into categories (e.g., age groups)  Creating binary indicators (e.g., high-risk vs. low-risk)  Recoding categorical variables (e.g., combining similar categories)      Temporal Derivations    Calculating time differences (e.g., days between events)  Creating growth rates (e.g., annual percentage change)  Extracting components from dates (e.g., month, day of week)       Derived Variables in Community Health   For our Community Health dataset, useful derived variables might include:    Environmental Quality Index : A weighted average combining air quality, water quality, and green space measures    Health Disparity Ratio : The ratio of health outcomes in highest-income versus lowest-income neighborhoods    Risk Categories : Classifying neighborhoods as \"high,\" \"medium,\" or \"low\" risk based on multiple environmental factors    Green Space per Capita : Total green space area divided by population    Walkability Score : Combining measures of sidewalk coverage, street connectivity, and proximity to amenities      Creating effective derived variables requires:    Clear definition of what you're trying to measure    Thoughtful selection of component variables    Appropriate mathematical operations    Verification that the derived variable behaves as expected    Documentation of how the variable was created     Matching Derived Variables   Match each derived variable with the most appropriate formula or method for creating it.     Body Mass Index (BMI)  weight (kg) \/ [height (m)]²    Age category  if(age < 18, \"Child\", if(age < 65, \"Adult\", \"Senior\"))    Percentage change  ((new_value - old_value) \/ old_value) * 100    Standardized score  (value - mean) \/ standard_deviation    High-risk indicator  if(risk_score > threshold, 1, 0)      Creating Derived Variables in CODAP   In this activity, you'll create and analyze derived variables in your dataset.     Create at least three derived variables in your dataset, including:    A mathematical transformation of an existing variable (e.g., log, square root, or standardization)    A ratio or relationship between two variables    A categorical variable derived from a numerical variable (e.g., binning into groups)        Create visualizations showing the relationships between your original variables and the derived variables.      Explore how your derived variables relate to other variables in the dataset. Do they reveal patterns that weren't obvious with the original variables?       Grouping and Aggregation Techniques  Grouping and aggregation allow us to summarize data at different levels and examine patterns across categories. These techniques help us answer questions about how metrics vary between groups.  Common grouping and aggregation operations include:    Group By  Dividing data into subsets based on categories (e.g., by region, income level, or time period).    Aggregate Functions  Calculating summary statistics for each group:  Count: Number of observations  Sum: Total of values  Average: Mean value  Min\/Max: Smallest\/largest values  Standard Deviation: Measure of spread      Pivot Tables  Reorganizing data to show aggregated values across multiple dimensions.    Hierarchical Grouping  Nesting groups within groups (e.g., neighborhoods within districts within cities).     Grouping and Aggregation in CODAP   Screenshot showing CODAP's interface for grouping data by categories and displaying aggregated metrics for each group.     Grouping in Community Health Analysis   In our Community Health dataset, we might use grouping and aggregation to:    Calculate average asthma rates by income quartile to examine socioeconomic health disparities    Compare environmental quality metrics across different regions of the city    Examine how multiple health indicators vary across neighborhoods with different levels of green space access    Create a pivot table showing average health metrics by both region and income level simultaneously    Calculate the standard deviation of air quality within each region to understand environmental variability       Purpose of Grouping and Aggregation   What is the PRIMARY purpose of grouping and aggregation in data analysis?      To remove outliers from a dataset    While aggregation may reduce the impact of outliers in summary statistics, this is not the primary purpose of grouping and aggregation.      To understand patterns and variations across categories or subsets    Correct! The primary purpose of grouping and aggregation is to reveal how metrics and patterns vary across different categories or subsets, allowing for meaningful comparisons.      To reduce the size of large datasets    While aggregation does create a more compact summary, this is a side effect rather than the primary purpose of grouping and aggregation.      To correct errors in the original data    Grouping and aggregation do not correct errors in the original data; in fact, they might obscure some errors by combining them with correct values.       Grouping and Aggregation in CODAP   In this activity, you'll practice grouping and aggregating data in CODAP.     Identify at least two categorical variables in your dataset that would be meaningful to group by.      For each grouping variable, create a summary table in CODAP that shows aggregated measures (mean, count, etc.) of at least two numerical variables for each group.      Create visualizations comparing these groups. Use bar charts or box plots to show how the numerical variables differ across categories.      Try creating a two-way grouping by two different categorical variables. Examine how this more detailed breakdown reveals patterns that might not be apparent in single-variable groupings.      "
},
{
  "id": "subsec-summary-statistics-3-1-2",
  "level": "2",
  "url": "sec-summarizing-calculating.html#subsec-summary-statistics-3-1-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Mean Median Mode "
},
{
  "id": "subsec-summary-statistics-3-2-2",
  "level": "2",
  "url": "sec-summarizing-calculating.html#subsec-summary-statistics-3-2-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Range Variance Standard Deviation Interquartile Range (IQR) "
},
{
  "id": "subsec-summary-statistics-3-3-2",
  "level": "2",
  "url": "sec-summarizing-calculating.html#subsec-summary-statistics-3-3-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Percentiles Quartiles Z-scores "
},
{
  "id": "subsec-summary-statistics-3-4-2",
  "level": "2",
  "url": "sec-summarizing-calculating.html#subsec-summary-statistics-3-4-2",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Correlation Covariance Contingency tables "
},
{
  "id": "fig-summary-stats-distributions",
  "level": "2",
  "url": "sec-summarizing-calculating.html#fig-summary-stats-distributions",
  "type": "Figure",
  "number": "71",
  "title": "",
  "body": " Summary Statistics for Different Distributions   Three different distributions showing how the same mean and standard deviation can represent very different data patterns.   "
},
{
  "id": "mc-summary-stats",
  "level": "2",
  "url": "sec-summarizing-calculating.html#mc-summary-stats",
  "type": "Checkpoint",
  "number": "72",
  "title": "Choosing Appropriate Summary Statistics.",
  "body": " Choosing Appropriate Summary Statistics   For a highly skewed distribution of housing prices in a city, which measure of central tendency would be MOST appropriate to report?      Mean (average) price    The mean is highly influenced by extreme values and can be misleading for skewed distributions. In housing prices, a few very expensive properties can pull the mean upward, making it unrepresentative of typical prices.      Median price    Correct! The median is the most appropriate measure for skewed distributions like housing prices because it's not influenced by extreme values. It represents the middle value, giving a better sense of the \"typical\" home price.      Modal price (most common price)    While the mode can be useful for categorical data, it's generally less informative for continuous variables like housing prices, which may not have many exact repeated values.      Midrange (average of minimum and maximum prices)    The midrange is highly influenced by outliers and would be particularly problematic for a skewed distribution of housing prices, where the maximum value could be dramatically higher than most values.     "
},
{
  "id": "activity-summary-stats",
  "level": "2",
  "url": "sec-summarizing-calculating.html#activity-summary-stats",
  "type": "Activity",
  "number": "21",
  "title": "Calculating Summary Statistics in CODAP.",
  "body": " Calculating Summary Statistics in CODAP   In this activity, you'll calculate and interpret summary statistics for your dataset.     Select at least three numerical variables from your dataset. For each variable, use CODAP to calculate:    Mean, median, and mode    Standard deviation and interquartile range    Minimum, maximum, and range        Create visualizations (histograms or box plots) for each variable and examine how the summary statistics relate to the distribution shape.      For each variable, determine which measure of central tendency (mean, median, or mode) best represents the \"typical\" value and explain why.      Calculate correlation coefficients between pairs of numerical variables. Identify the strongest positive and negative correlations in your dataset.    "
},
{
  "id": "example-derived-health",
  "level": "2",
  "url": "sec-summarizing-calculating.html#example-derived-health",
  "type": "Example",
  "number": "73",
  "title": "Derived Variables in Community Health.",
  "body": " Derived Variables in Community Health   For our Community Health dataset, useful derived variables might include:    Environmental Quality Index : A weighted average combining air quality, water quality, and green space measures    Health Disparity Ratio : The ratio of health outcomes in highest-income versus lowest-income neighborhoods    Risk Categories : Classifying neighborhoods as \"high,\" \"medium,\" or \"low\" risk based on multiple environmental factors    Green Space per Capita : Total green space area divided by population    Walkability Score : Combining measures of sidewalk coverage, street connectivity, and proximity to amenities     "
},
{
  "id": "matching-derived-variables",
  "level": "2",
  "url": "sec-summarizing-calculating.html#matching-derived-variables",
  "type": "Checkpoint",
  "number": "74",
  "title": "Matching Derived Variables.",
  "body": " Matching Derived Variables   Match each derived variable with the most appropriate formula or method for creating it.     Body Mass Index (BMI)  weight (kg) \/ [height (m)]²    Age category  if(age < 18, \"Child\", if(age < 65, \"Adult\", \"Senior\"))    Percentage change  ((new_value - old_value) \/ old_value) * 100    Standardized score  (value - mean) \/ standard_deviation    High-risk indicator  if(risk_score > threshold, 1, 0)    "
},
{
  "id": "activity-derived-variables",
  "level": "2",
  "url": "sec-summarizing-calculating.html#activity-derived-variables",
  "type": "Activity",
  "number": "22",
  "title": "Creating Derived Variables in CODAP.",
  "body": " Creating Derived Variables in CODAP   In this activity, you'll create and analyze derived variables in your dataset.     Create at least three derived variables in your dataset, including:    A mathematical transformation of an existing variable (e.g., log, square root, or standardization)    A ratio or relationship between two variables    A categorical variable derived from a numerical variable (e.g., binning into groups)        Create visualizations showing the relationships between your original variables and the derived variables.      Explore how your derived variables relate to other variables in the dataset. Do they reveal patterns that weren't obvious with the original variables?    "
},
{
  "id": "fig-grouping-aggregation",
  "level": "2",
  "url": "sec-summarizing-calculating.html#fig-grouping-aggregation",
  "type": "Figure",
  "number": "75",
  "title": "",
  "body": " Grouping and Aggregation in CODAP   Screenshot showing CODAP's interface for grouping data by categories and displaying aggregated metrics for each group.   "
},
{
  "id": "example-grouping-health",
  "level": "2",
  "url": "sec-summarizing-calculating.html#example-grouping-health",
  "type": "Example",
  "number": "76",
  "title": "Grouping in Community Health Analysis.",
  "body": " Grouping in Community Health Analysis   In our Community Health dataset, we might use grouping and aggregation to:    Calculate average asthma rates by income quartile to examine socioeconomic health disparities    Compare environmental quality metrics across different regions of the city    Examine how multiple health indicators vary across neighborhoods with different levels of green space access    Create a pivot table showing average health metrics by both region and income level simultaneously    Calculate the standard deviation of air quality within each region to understand environmental variability     "
},
{
  "id": "mc-grouping-purpose",
  "level": "2",
  "url": "sec-summarizing-calculating.html#mc-grouping-purpose",
  "type": "Checkpoint",
  "number": "77",
  "title": "Purpose of Grouping and Aggregation.",
  "body": " Purpose of Grouping and Aggregation   What is the PRIMARY purpose of grouping and aggregation in data analysis?      To remove outliers from a dataset    While aggregation may reduce the impact of outliers in summary statistics, this is not the primary purpose of grouping and aggregation.      To understand patterns and variations across categories or subsets    Correct! The primary purpose of grouping and aggregation is to reveal how metrics and patterns vary across different categories or subsets, allowing for meaningful comparisons.      To reduce the size of large datasets    While aggregation does create a more compact summary, this is a side effect rather than the primary purpose of grouping and aggregation.      To correct errors in the original data    Grouping and aggregation do not correct errors in the original data; in fact, they might obscure some errors by combining them with correct values.     "
},
{
  "id": "activity-grouping",
  "level": "2",
  "url": "sec-summarizing-calculating.html#activity-grouping",
  "type": "Activity",
  "number": "23",
  "title": "Grouping and Aggregation in CODAP.",
  "body": " Grouping and Aggregation in CODAP   In this activity, you'll practice grouping and aggregating data in CODAP.     Identify at least two categorical variables in your dataset that would be meaningful to group by.      For each grouping variable, create a summary table in CODAP that shows aggregated measures (mean, count, etc.) of at least two numerical variables for each group.      Create visualizations comparing these groups. Use bar charts or box plots to show how the numerical variables differ across categories.      Try creating a two-way grouping by two different categorical variables. Examine how this more detailed breakdown reveals patterns that might not be apparent in single-variable groupings.    "
},
{
  "id": "sec-statistical-thinking-comparing",
  "level": "1",
  "url": "sec-statistical-thinking-comparing.html",
  "type": "Section",
  "number": "",
  "title": "Statistical Thinking: Comparing Groups",
  "body": " Statistical Thinking: Comparing Groups   Variation Within and Between Groups  When comparing groups, it's important to consider both the variation within each group and the variation between groups. This helps us determine whether observed differences are meaningful or might simply be due to random variation.     Within-group variation refers to how much values differ from each other within the same group or category.       Between-group variation refers to how much the typical values (e.g., means) differ across different groups or categories.     Within- and Between-Group Variation   Two scenarios: one showing large between-group differences with small within-group variation, and another showing small between-group differences with large within-group variation.    The relationship between within-group and between-group variation affects our ability to draw meaningful conclusions:    When between-group variation is large relative to within-group variation, group differences are more likely to be meaningful.    When within-group variation is large relative to between-group variation, apparent group differences might just reflect random variation.     Variation in Community Health Data   When comparing asthma rates across income levels in our Community Health dataset:    Within-group variation : The range of asthma rates among neighborhoods within the same income category (e.g., high-income neighborhoods might have asthma rates ranging from 5% to 12%).    Between-group variation : The difference in average asthma rates between income categories (e.g., high-income neighborhoods averaging 8% versus low-income neighborhoods averaging 15%).    If within-group variation is small (neighborhoods within the same income category have similar asthma rates) and between-group variation is large (different income categories have noticeably different average asthma rates), we might reasonably conclude that income level is associated with asthma prevalence.     Comparing Group Variations   Based on the box plots below, which statement is most accurate?  (Imagine a figure showing box plots of test scores for three teaching methods: A, B, and C. Method A has scores ranging from 60-80 with median 70, Method B has scores ranging from 65-85 with median 75, and Method C has scores ranging from 40-95 with median 72.)      Method C is clearly the most effective teaching approach.    This is not supported by the data. While Method C has some high scores, it also has the lowest scores and the widest range, indicating inconsistent results.      There are no meaningful differences between the teaching methods.    This overlooks the notable differences in both median scores and score distributions among the methods.      Method B has the highest median score, but the differences between methods are modest.    This accurately notes Method B's higher median but doesn't address the important difference in variability.      Method B shows a higher median with relatively low variability, while Method C shows inconsistent results with high variability.    Correct! This statement accurately describes both the differences in central tendency (median scores) and the critical difference in within-group variation, with Method C showing much higher variability in outcomes than Methods A and B.        Making Meaningful Comparisons  To make meaningful comparisons between groups, consider these key principles:    Compare Like with Like  Ensure that groups are comparable in terms of relevant characteristics other than the one you're studying.    Consider Sample Size  Larger groups generally provide more reliable estimates than smaller groups.    Examine Both Summary Statistics and Distributions  Don't rely solely on averages; consider the full distribution of values within each group.    Visualize Comparisons  Use appropriate visualizations (box plots, bar charts with error bars, etc.) to show both central tendency and variation.    Test for Statistical Significance  When appropriate, use statistical tests to assess whether differences are likely due to chance.    Consider Practical Significance  Even statistically significant differences might not be practically meaningful if they're very small.     Visualizing Group Comparisons   Various visualization types for comparing groups: box plots, bar charts with error bars, and overlaid distributions.     Meaningful Comparisons in Community Health   To meaningfully compare asthma rates between high-income and low-income neighborhoods in our dataset, we might:    Control for other factors by comparing neighborhoods with similar population density, age distribution, and geographic location    Ensure we have enough neighborhoods in each income category for reliable comparison    Examine not just average asthma rates but also the range and distribution within each income group    Create box plots showing asthma rates by income category, clearly displaying both central tendency and variation    Conduct a statistical test (e.g., t-test) to assess whether the difference in means is statistically significant    Consider whether the observed difference in asthma rates (e.g., 7 percentage points) is large enough to be medically and socially significant       Comparing Groups in Your Dataset   In this activity, you'll practice making meaningful comparisons between groups in your dataset.     Identify a categorical variable in your dataset that creates meaningful groups for comparison. This could be a variable that was in the original dataset or a derived categorical variable you created.      Select at least two numerical variables to compare across these groups.      Create appropriate visualizations (box plots, bar charts with measures of variation, etc.) to compare the groups.      Write a brief analysis of the comparisons, addressing:    What differences do you observe between groups?    How much variation exists within each group?    Are the differences large enough to be meaningful in the context of your research questions?    What factors might explain the differences you observed?        Group Comparison Pitfalls   For each scenario, identify the primary issue that could lead to misleading conclusions when comparing groups.     A researcher compares the performance of students who chose to participate in an optional after-school program with those who did not.      Inadequate sample size    The scenario doesn't mention sample size, so this isn't the primary issue.      Self-selection bias    Correct! Students who choose to participate in optional programs may already be more motivated or higher-performing than those who don't. This self-selection bias means the groups differ in ways beyond just program participation.      Comparing unlike time periods    The scenario doesn't involve time period comparisons, so this isn't the issue.      Using inappropriate statistical tests    The scenario doesn't mention what statistical tests were used, so this isn't identified as the primary issue.        An analyst reports that neighborhoods with more parks have lower crime rates, suggesting that parks reduce crime.      Insufficient visualization of the data    While visualization is important, the primary issue here is about the conclusion being drawn rather than how the data is presented.      High within-group variation    The scenario doesn't mention variation within groups, so this isn't identified as the primary issue.      Confusing correlation with causation    Correct! The analyst observes a correlation (neighborhoods with more parks have lower crime) but suggests causation (parks reduce crime). Other factors like neighborhood wealth, housing density, or policing could explain both the presence of parks and lower crime rates.      Outlier influence    The scenario doesn't mention outliers, so this isn't identified as the primary issue.        "
},
{
  "id": "def-within-group-variation",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#def-within-group-variation",
  "type": "Definition",
  "number": "78",
  "title": "",
  "body": "   Within-group variation refers to how much values differ from each other within the same group or category.   "
},
{
  "id": "def-between-group-variation",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#def-between-group-variation",
  "type": "Definition",
  "number": "79",
  "title": "",
  "body": "   Between-group variation refers to how much the typical values (e.g., means) differ across different groups or categories.   "
},
{
  "id": "fig-within-between-variation",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#fig-within-between-variation",
  "type": "Figure",
  "number": "80",
  "title": "",
  "body": " Within- and Between-Group Variation   Two scenarios: one showing large between-group differences with small within-group variation, and another showing small between-group differences with large within-group variation.   "
},
{
  "id": "example-variation-health",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#example-variation-health",
  "type": "Example",
  "number": "81",
  "title": "Variation in Community Health Data.",
  "body": " Variation in Community Health Data   When comparing asthma rates across income levels in our Community Health dataset:    Within-group variation : The range of asthma rates among neighborhoods within the same income category (e.g., high-income neighborhoods might have asthma rates ranging from 5% to 12%).    Between-group variation : The difference in average asthma rates between income categories (e.g., high-income neighborhoods averaging 8% versus low-income neighborhoods averaging 15%).    If within-group variation is small (neighborhoods within the same income category have similar asthma rates) and between-group variation is large (different income categories have noticeably different average asthma rates), we might reasonably conclude that income level is associated with asthma prevalence.   "
},
{
  "id": "mc-variation-comparison",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#mc-variation-comparison",
  "type": "Checkpoint",
  "number": "82",
  "title": "Comparing Group Variations.",
  "body": " Comparing Group Variations   Based on the box plots below, which statement is most accurate?  (Imagine a figure showing box plots of test scores for three teaching methods: A, B, and C. Method A has scores ranging from 60-80 with median 70, Method B has scores ranging from 65-85 with median 75, and Method C has scores ranging from 40-95 with median 72.)      Method C is clearly the most effective teaching approach.    This is not supported by the data. While Method C has some high scores, it also has the lowest scores and the widest range, indicating inconsistent results.      There are no meaningful differences between the teaching methods.    This overlooks the notable differences in both median scores and score distributions among the methods.      Method B has the highest median score, but the differences between methods are modest.    This accurately notes Method B's higher median but doesn't address the important difference in variability.      Method B shows a higher median with relatively low variability, while Method C shows inconsistent results with high variability.    Correct! This statement accurately describes both the differences in central tendency (median scores) and the critical difference in within-group variation, with Method C showing much higher variability in outcomes than Methods A and B.     "
},
{
  "id": "fig-meaningful-comparison",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#fig-meaningful-comparison",
  "type": "Figure",
  "number": "83",
  "title": "",
  "body": " Visualizing Group Comparisons   Various visualization types for comparing groups: box plots, bar charts with error bars, and overlaid distributions.   "
},
{
  "id": "example-comparison-health",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#example-comparison-health",
  "type": "Example",
  "number": "84",
  "title": "Meaningful Comparisons in Community Health.",
  "body": " Meaningful Comparisons in Community Health   To meaningfully compare asthma rates between high-income and low-income neighborhoods in our dataset, we might:    Control for other factors by comparing neighborhoods with similar population density, age distribution, and geographic location    Ensure we have enough neighborhoods in each income category for reliable comparison    Examine not just average asthma rates but also the range and distribution within each income group    Create box plots showing asthma rates by income category, clearly displaying both central tendency and variation    Conduct a statistical test (e.g., t-test) to assess whether the difference in means is statistically significant    Consider whether the observed difference in asthma rates (e.g., 7 percentage points) is large enough to be medically and socially significant     "
},
{
  "id": "activity-group-comparison",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#activity-group-comparison",
  "type": "Activity",
  "number": "24",
  "title": "Comparing Groups in Your Dataset.",
  "body": " Comparing Groups in Your Dataset   In this activity, you'll practice making meaningful comparisons between groups in your dataset.     Identify a categorical variable in your dataset that creates meaningful groups for comparison. This could be a variable that was in the original dataset or a derived categorical variable you created.      Select at least two numerical variables to compare across these groups.      Create appropriate visualizations (box plots, bar charts with measures of variation, etc.) to compare the groups.      Write a brief analysis of the comparisons, addressing:    What differences do you observe between groups?    How much variation exists within each group?    Are the differences large enough to be meaningful in the context of your research questions?    What factors might explain the differences you observed?      "
},
{
  "id": "exercise-comparison-pitfalls",
  "level": "2",
  "url": "sec-statistical-thinking-comparing.html#exercise-comparison-pitfalls",
  "type": "Checkpoint",
  "number": "85",
  "title": "Group Comparison Pitfalls.",
  "body": " Group Comparison Pitfalls   For each scenario, identify the primary issue that could lead to misleading conclusions when comparing groups.     A researcher compares the performance of students who chose to participate in an optional after-school program with those who did not.      Inadequate sample size    The scenario doesn't mention sample size, so this isn't the primary issue.      Self-selection bias    Correct! Students who choose to participate in optional programs may already be more motivated or higher-performing than those who don't. This self-selection bias means the groups differ in ways beyond just program participation.      Comparing unlike time periods    The scenario doesn't involve time period comparisons, so this isn't the issue.      Using inappropriate statistical tests    The scenario doesn't mention what statistical tests were used, so this isn't identified as the primary issue.        An analyst reports that neighborhoods with more parks have lower crime rates, suggesting that parks reduce crime.      Insufficient visualization of the data    While visualization is important, the primary issue here is about the conclusion being drawn rather than how the data is presented.      High within-group variation    The scenario doesn't mention variation within groups, so this isn't identified as the primary issue.      Confusing correlation with causation    Correct! The analyst observes a correlation (neighborhoods with more parks have lower crime) but suggests causation (parks reduce crime). Other factors like neighborhood wealth, housing density, or policing could explain both the presence of parks and lower crime rates.      Outlier influence    The scenario doesn't mention outliers, so this isn't identified as the primary issue.      "
},
{
  "id": "sec-unit3-summary",
  "level": "1",
  "url": "sec-unit3-summary.html",
  "type": "Section",
  "number": "",
  "title": "Unit 3 Summary",
  "body": " Unit 3 Summary  In this unit, we've explored essential data moves that transform raw data into meaningful insights:    Data Cleaning and Organization : We learned how to handle missing values, deal with outliers, and restructure data to prepare it for analysis.    Filtering and Subsetting : We explored techniques for creating focused subsets of data to answer specific questions and make meaningful comparisons.    Summarizing and Calculating : We examined summary statistics that capture key aspects of our data and learned to create derived variables that reveal new insights.    Grouping and Comparing : We developed skills for aggregating data by categories and making meaningful comparisons between groups.    We also explored important ethical considerations regarding selection bias and developed our statistical thinking about variation within and between groups.  By the end of this unit, you should have applied these data moves to both our Community Health dataset and your own chosen dataset. These skills provide the foundation for the more advanced visualization and communication techniques we'll explore in the next unit.   Unit 3 Reflection   Take some time to reflect on what you've learned in this unit:    Which data move did you find most challenging to implement, and why?    What surprised you about your dataset when you applied these data moves?    How have these data moves helped you address your investigation questions?    What new questions have emerged as you've worked with your data?       Unit 3 Review   Which sequence of data moves best represents a typical data analysis workflow?      Grouping → Cleaning → Filtering → Creating visualizations    This sequence is problematic because grouping data before cleaning it could lead to incorrect aggregations based on errors or missing values.      Cleaning → Creating derived variables → Filtering → Grouping and comparing    Correct! This sequence represents a logical workflow: first clean the data to address quality issues, then create any needed derived variables, then filter to focus on relevant subsets, and finally group and compare to identify patterns.      Filtering → Cleaning → Summarizing → Creating derived variables    Filtering before cleaning could result in removing data that might be valuable once cleaned, potentially introducing bias.      Creating derived variables → Summarizing → Cleaning → Grouping    Creating derived variables before cleaning could propagate errors into new variables, and summarizing before cleaning could lead to misleading statistics.      "
},
{
  "id": "sec-unit3-summary-3-1-1",
  "level": "2",
  "url": "sec-unit3-summary.html#sec-unit3-summary-3-1-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Data Cleaning and Organization "
},
{
  "id": "sec-unit3-summary-3-2-1",
  "level": "2",
  "url": "sec-unit3-summary.html#sec-unit3-summary-3-2-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Filtering and Subsetting "
},
{
  "id": "sec-unit3-summary-3-3-1",
  "level": "2",
  "url": "sec-unit3-summary.html#sec-unit3-summary-3-3-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Summarizing and Calculating "
},
{
  "id": "sec-unit3-summary-3-4-1",
  "level": "2",
  "url": "sec-unit3-summary.html#sec-unit3-summary-3-4-1",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "Grouping and Comparing "
},
{
  "id": "exercise-unit3-reflection",
  "level": "2",
  "url": "sec-unit3-summary.html#exercise-unit3-reflection",
  "type": "Checkpoint",
  "number": "86",
  "title": "Unit 3 Reflection.",
  "body": " Unit 3 Reflection   Take some time to reflect on what you've learned in this unit:    Which data move did you find most challenging to implement, and why?    What surprised you about your dataset when you applied these data moves?    How have these data moves helped you address your investigation questions?    What new questions have emerged as you've worked with your data?     "
},
{
  "id": "mc-unit3-review",
  "level": "2",
  "url": "sec-unit3-summary.html#mc-unit3-review",
  "type": "Checkpoint",
  "number": "87",
  "title": "Unit 3 Review.",
  "body": " Unit 3 Review   Which sequence of data moves best represents a typical data analysis workflow?      Grouping → Cleaning → Filtering → Creating visualizations    This sequence is problematic because grouping data before cleaning it could lead to incorrect aggregations based on errors or missing values.      Cleaning → Creating derived variables → Filtering → Grouping and comparing    Correct! This sequence represents a logical workflow: first clean the data to address quality issues, then create any needed derived variables, then filter to focus on relevant subsets, and finally group and compare to identify patterns.      Filtering → Cleaning → Summarizing → Creating derived variables    Filtering before cleaning could result in removing data that might be valuable once cleaned, potentially introducing bias.      Creating derived variables → Summarizing → Cleaning → Grouping    Creating derived variables before cleaning could propagate errors into new variables, and summarizing before cleaning could lead to misleading statistics.     "
},
{
  "id": "backmatter-2",
  "level": "1",
  "url": "backmatter-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')
  this.metadataWhitelist = ['position']

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
